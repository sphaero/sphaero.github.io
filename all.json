{"_rd_": {"coordspace": {"2d.dia": "_rd_/coordspace", "_abstract": "In this paper we argue for a unification of coordinate spaces for use in mixed reality setups. The paper introduces most used coordinate systems and proposes a system for unification based on existing coordinate systems. An example is given at the end of the paper.\n", "_tags": "zocp, R&D, lab, workflow\n", "_author": "Arnaud Loonstra, arnaud@sphaero.org, Stichting z25.org\n", "coordspace.pdf": "_rd_/coordspace", "2d.png": {"web": "2d_web.jpg", "thumb": "2d_thumb.jpg", "md5": "todo"}, "1d.dia": "_rd_/coordspace", "1d.png": {"web": "1d_web.jpg", "thumb": "1d_thumb.jpg", "md5": "todo"}, "_body.textile": "\t<h2>1. Introduction</h2>\n\n\t<p>Our environment is nowadays filled with technology. Mobile phones, smart watches, game interfaces, sensors, camera&#8217;s, rfid, etc. This means all this technology can somehow relate to the physical realm. However there are no standards for mapping a physical realm to a digital/virtual realm. One can create a mapping like one needs. However when the amount of devices grows this can be a challenging task. You hope you chose a sensible and flexible mapping set for every device. Apart from making it work it would be more beneficial if the mapping is 'making sense&#8217;. If coordinate systems would not map 1 on 1 then for every change one would need to think about the translation from one coordinate system into the other. This is unintuitive, error prone and making adoption of a clear mental model more difficult. <br />In this document we outline the results of our research into homogenizing different coordinates systems from the physical and digital realm. We propose a mapping system to homogenize the realm&#8217;s coordinate systems. The proposed mapping is tested for use in the <span class=\"caps\">ZOCP</span> protocol<sup class=\"footnote\"><a href=\"#fn4df33235-27b3-4daa-9b67-5238066e8523\">1</a></sup>.</p>\n\n\t<h2>2. What is a coordinate system</h2>\n\n\t<p>A coordinate system is a numbering system to uniquely determine a position in a space. A simple example of coordinate system is a position on a line. When an origin, position 0, is determined we can relate the position of any unique point on that line to a positive or negative number. <br />When we want to determine a position on a plane we need to add another line to determine the position using 2 numbers. We call this a 2 dimensional coordinate system. <br />We can repeat this to position into 3 dimensions. This can continue indefinitely. However in our physical presence we are used to a three dimensional space. Sometimes we add time as a fourth dimensional coordinate. It is important to understand that the limitation of our physical presence to 3 dimensions is not valid for mathematical spaces. <br />Besides describing linear positions we can also define angular positions which are usually used for describing orientations. Just think about a compass. It defines North, South, East and West directions. It also defines this as numbers North being 0 degrees and South being 180 degrees. Again this can be repeated to multiple orientation dimensions. We will describe this in detail in this paper.</p>\n\n\t<h2>3. Physical coordinate systems</h2>\n\n\t<p>In our daily life we are used to many different coordinate systems. When we drive our car to work we are used to kilometers in most countries but in others miles might be used. When sail a ship we will probably use nautical miles and perhaps when we are flying to another planet we are talking about light years. All these coordinate systems are essentially the same. They only differ in the units they use. We will describe the most used coordinate systems and reason why we chose to use them.</p>\n\n\t<h3>3.1. Metric System</h3>\n\n\t<p>The Metric System<sup class=\"footnote\"><a href=\"#fna56d76ca-bb15-4090-bc13-757a34593c23\">2</a></sup> is a system introduced by the French in 1799. Nowadays it is also used under the synonym \u201cSI\u201d or \u201cInternational System of Units\u201d. It is officially the most used measuring system in almost all countries. This is also the most profound reason why we chose to adopt this system as a standard for mapping physical and virtual realms. The only countries that do not use the metric system are America, Liberia and Myanmar(Burma). <br />The foundations of the metric system are the 'meter&#8217; and the 'kilogram&#8217;. The metric system was designed to be universal and based on natural phenomena. For example one meter was initially defined to be one ten millionth of the distance from the equator to the North Pole through Paris. A kilogram was defined as the mass of 1/1000th of a cubic meter of pure water at its melting point. Future developments showed more accurate ways to determine a meter&#8217;s length and by 1983 a meter was defined by the constant of the speed of light. <br />It is important to realize that these metric units are based on natural reproducible facts. Many other units are based on the metric system and therefore show coherence. We will show later that this is important for the digital realm as well. Since the metric system enjoys broad adoption we will adopt this system as a foundation.<br />In 1999 the $125 million dollar Mars Climate Orbiter crashed into Mars because the American spacecraft engineers calculated using American units while the thrust engineers were expecting units specified in the metric system. This is illustrates the importance of having agreed standards hence this research.</p>\n\n\t<h3>3.2. Orientation</h3>\n\n\t<p>The most common orientation system is the compass bearing. It was invented somewhere around 200BC and we are still using the North, South, East, West orientations today. The compass bearing describes a single dimensional orientation using degrees as an angular unit. It is unknown how the degree unit originated and what it is based on. The degree is not an SI unit. It is however mentioned as a 'accepted&#8217; unit. The official unit in SI is a 'radian&#8217;. The radian is a more practical unit since it is easier to use for mathematical operations. The radian is defined as being the length of half a circle with a radius of 1. So if you take half a circle with a one meter radius the length of this half circle will be 1 radian which is 3.1415926535&#8230; meter. This is the famous Pi (\u03c0) number. Illustration 3 is a chart consisting of different angular coordinates and show how to convert between them. <br />Ever since we are looking up to the stars we know that a compass bearing orientation is not enough to describe our orientation. There are at least three units needed to describe our orientation. These are often called yaw, pitch and roll and stem from Euler Angles. Other system might refer to these as heading, pitch and bank. As both define pitch this can de the source of confusion hence we discard this notation. <br />An Euler Angle has its limitations. Imagine turning to an orientation with the maximum values. Now try turning beyond that orientation. This called the 'Gimbal Lock&#8217;. Because of the 'Gimbal Lock&#8217; quaternion orientation might be suited better. As Quaternions are very mathematical we only mention their existence as a coordinate orientation and will not elaborate further<sup class=\"footnote\"><a href=\"#fn77fd39c5-9ae4-40dc-b902-f952bae5e408\">3</a></sup>.</p>\n\n\t<h3>3.3. Time</h3>\n\n\t<p>Time is one of the seven fundamentals in the metric system (SI). Time is used to describe the order of events from past to present to future. It is thus a one dimensional coordinate system. A time unit in the metric system is defined as a second. Initially time was determined by the cycle of the moon and the sun but nowadays a second is defined in terms of radiation emitted by caesium atoms.</p>\n\n\t<h3>3.4. Positioning on earth</h3>\n\n\t<p>A coordinate system known by many because of its adoption by navigations devices using the Global Positioning System (<span class=\"caps\">GPS</span>) is called WGS84<sup class=\"footnote\"><a href=\"#fn751a30b4-d5f3-4f3d-aa1c-8fb18c7835ac\">4</a></sup>. It was established in 1984 and its latest revision is of 2004. WGS84 is a coordinate system to describe any location on earth with a 2cm accuracy.  It is important to understand that WGS84 is a 2D coordinate system on a mathematical sphere. This means it can never describe the height of your position. As WGS84, through the <span class=\"caps\">GPS</span> system, has broad adoption it would be unwise to not adopt it. However it is wise to look further as one could imagine doing something outside of earths boundaries (e.g. satellites, space probes). Some alternatives to WGS84 are the European Terrestrial Reference System 1989 and North American Datum 1983 however these have the same shortcoming as WGS84. The International Astronomical Union (<span class=\"caps\">IAU</span>) has adopted the International Celestial Reference System (<span class=\"caps\">ICRS</span>). <span class=\"caps\">ICRS</span> is a high precision positional coordinate system. It has its origin (position 0,0,0) at the solar system barycenter. This would be an ideal reference coordinate system. On the other hand to use it practically will be very hard because of the simple fact that our planet earth is flying around the sun at an incredible speed. Any <span class=\"caps\">ICRS</span> coordinate on earth would be subject to a constant change and a fixed position depends on time. So to remain practical we would adopt the common coordinate system used by <span class=\"caps\">GPS</span> receivers, but to remain ambitious we would adopt the <span class=\"caps\">ICRS</span> coordinate system</p>\n\n\t<h2>4. Virtual coordinate systems</h2>\n\n\t<p>The virtual or digital realm is essentially an abstraction. Inside the computer there is no such thing as a virtual reality that can be regarded as the physical reality. The virtual reality can computationally simulate a physical reality in 3 dimensions but since it is an abstraction the computer can continue beyond three dimensions. However for our purposes of mapping a physical reality to a virtual reality three dimension will do fine. We will look into coordinate systems similar to those in the physical realm used in the digital realms. As most of our digital realities are related to OpenGL it might be sensible to follow its approach as close as possible. </p>\n\n\t<h3>4.1. OpenGL</h3>\n\n\t<p>The most used functionality of OpenGL is to translate a three dimensional scene into a two dimensional representation. This is basically what happens when you are playing a 3D game on your monitor. OpenGL defines several coordinate spaces to support converting a 3d space into a 2d representation. However OpenGL, being a general purpose language, does not dictate any standard into mapping a physical coordinate space to a virtual. It therefore provides any desired way hence our need to settle on one approach to prevent dealing with different interpretations. <br />OpenGL defines \u201cObject Space\u201d. Three-dimensional objects are defined a coordinate space convenient for describing the object&#8217;s properties (vertices, normals etc). Or to put in layman&#8217;s terms; in case of a pyramid the 5 points (4 bottom, 1 top) of the pyramid are described in \u201cObject Space\u201d coordinates.<br />To see this pyramid in a context of a scene OpenGL defines \u201cWorld Space\u201d. The pyramid&#8217;s location is described in \u201cWorld Space\u201d coordinates. <br />In order for the pyramid&#8217;s position to be described in this space it needs an 'origin&#8217; point. The origin point is described in the object&#8217;s \u201cObject Space\u201d. To understand this from a physical perspective imagine you are holding the pyramid in your hand. The exact position where you are holding the pyramid is the pyramid&#8217;s origin.<br />OpenGL has a notion of 'units&#8217;. However, if an object has a size of 3 units this could mean anything. Is a unit a meter, a millimeter, a lightyear?  To OpenGL this is unimportant however if one wants to relate to our physical space in an absolute way we need to specify what a 'unit&#8217; is. In OpenGL a unit can be different in every coordinate space. Suppose your pyramid has a unit size of 1 centimeter. In openGL it is possible to define \u201cWorld Space\u201d in meters. In order to have a correct representation of the pyramid we need to transform the \u201cObject Space\u201d coordinate system into the \u201cWorld Space\u201d coordinate system. In OpenGL this is called \u201cModel Transformation\u201d. In our example we need to scale the \u201cObject Space\u201d coordinate system by a factor of 10 to match it to \u201cWorld Space\u201d coordinates. In OpenGL the \u201cModel Transformation Matrix\u201d describes this transformation.   It is called a \u201cMatrix\u201d since the transformation between coordinate space is done using mathematical matrices. To understand this fully you have to understand linear algebra. However, at least remember that matrix calculation is used to transform from one coordinate space to another.<br />Finally in case we are displaying this openGL setup we need to transform \u201cWorld Space\u201d into \u201cEye Space\u201d. This is similar to \u201cObject Space\u201d transformation thus described in the \u201cView Matrix\u201d. To imagine this you need just need to think of the camera filming the scene. The camera has a position, orientation, etc in \u201cWorld Space\u201d coordinates. To display a scene through the camera all coordinates must be described in coordinate space of the camera.<br />To map the OpenGL coordinate system to a physical coordinate system we need to take some physical properties into account. We all know that when we hold a apple in the air and let go it will drop. You should know that this is how Newton discovered the basic laws of gravitation. The apple will fall to the floor with an acceleration of 9.81 m/s2. Suppose we would want to simulate this in openGL. We could write our own methods to simulate dropping an apple but we will probably use a physics engine as we don&#8217;t want to reinvent the wheel. A physics engine simulates essential properties of the physical realm. It would make sense to make use of the same gravitational laws as we have in the physical realm. Since we will probably remain on earth most of the time it makes sense to use the acceleration value of 9.81 m/s2. This implies we are calculating in meters. Therefore it makes sense to set an OpenGL unit to 1 meter as well to guarantee numbers remain the same. It is convenient to have equal number so we don&#8217;t have to transform numbers for each coordinate system. <br />Finally we need to define what is the positive direction and what negative. There are actually two main coordinate systems: Right-Handed and Left-Handed. Why those names? \n\t<ul>\n\t\t<li>1. Stretch your right arm and form a 90 degree angle with your elbow</li>\n\t\t<li>2. Point your right hand thumb to the right side</li>\n\t\t<li>3. Point your right hand finger up</li>\n\t\t<li>4. Point your right hand middle finger in your direction</li>\n\t</ul><br />This is the Right-handed coordinate system. Your fingers are now pointing in the positive direction. <br />OpenGL uses the Right-Handed system. Obviously if you would do this with your left hand you would get the Left-Handed system.<br />Actually we haven&#8217;t set this for our physical coordinate system either. To match directions in the physical realm we have to face north. By this the <span class=\"caps\">RHS</span> will be valid for the physical realm as well as this matches WGS84 directions. <br />We now have described the coordinate systems of OpenGL we need. We have \u201cWorld Space\u201d coordinates to define the location of every object in our world. We have \u201cObject Space\u201d coordinates to define the dimensional properties of an individual object. Why two sets of coordinates? Imagine your newly bought <span class=\"caps\">DIY</span> construction cupboard. If you are constructing the cupboard you are not thinking of how it is constructed in dimensional terms of your living room. The manufacturer also cannot give you instructions based on the dimensions of your living room because of the simple fact that the manufacturer doesn&#8217;t know the dimensions of your living room.</p>\n\n\t<h3>4.2. Digital Time</h3>\n\n\t<p>Time in a computer is a more complicated coordinate than in the physical realm. If you switch a computer off it has no notion of time. If you power it on it cannot know what time it is. For the computer time is a very relative coordinate. The only way it can determine a time unit is by its cpu clock. However this is not the most stable reference and often causes drift. Therefore most computers are equipped with an on-board clock. This clock ticks even if your computer is off. Even then time is unreliable as it must be set manually. Networked computers are able synchronize their clocks using a protocol like the Network Time Protocol (<span class=\"caps\">NTP</span>)[5]. This protocol ensures the computers have their clocks synchronized with maximally a few milliseconds drift. However without an external source time can still not be reliably mapped onto physical time. On internet a hierarchy of <span class=\"caps\">NTP</span> servers provide a reliable source to synchronize clocks with. The <span class=\"caps\">GPS</span> system (WGS84) also provides a very reliable time signal with an accuracy of +-10 nanoseconds. </p>\n\n\t<h2>5. Mapping physical coordinates to virtual</h2>\n\n\t<p>We believe that having a sensible coordinate systems improves workflow when working in both  realms. As said before transforming coordinates systems is an error prone activity as was illustrated by the crash of the Mars Climate Orbiter. We have experienced situations in which coordinate systems needed translation causing a lot of strain on rapid prototyping developments and situations which demanded improvising. Non-homogenized coordinate systems also proved much harder to debug again straining development progression. If we settle on the coordinate system provided by the metric system and the coordinate systems of OpenGL as described before we can now make a mapping between the physical and virtual realm. <br />We now describe the used coordinates starting in the physical realm and moving into the virtual realm. We are describing the coordinates as a hierarchy. When you have a chair in a room and you move the room the chair will move along. The relation of the objects is described by this hierarchy.<br />Our physical setup consists of a room with dimension of 3&#215;4&#215;2 meters (LxWxH) with its origin at <span class=\"caps\">GPS</span> coordinates: 52.1153016,5.1389998. We call this our physical root.</p>\n\n\t<p>root:\n WGS84: (52.1153016, 5.1389998)\n Room: \n  position: (0,0,0)\n  dimension: (2.0, 4.0, 2.0)\n  orientation: (radians? Euler?)\n  matrix: (describes, position, rotation and \n           scale in 9 floats)<br />Inside the room we put a chair. The chair has a dimension of 60&#215;80&#215;90 (LxWxH) centimeter and is placed in the left corner of the room opposite the origin of the room.\n  Chair:    \n   position: (1,5, 3.5, 0)\n   dimension: (0.6, 0.8, 0.9)\n   orientation: (0, 3.14, 0)<br />Also a television cannot be left out from the room is it already too dull. We place the television in the room facing the chair.\n  Television:\n   position: (1.0, 2.0, 0.7)\n   dimension: (0.8, 0.2, 0,45)\n   orientation: (0, -3,0, 0)<br />A television is an interesting object. The German word (fernseher) can be literally translated as 'seeing far&#8217;. The television is an object that can make us perceive a virtual world by vision. Usually a film displays it&#8217;s own perspective of a virtual world with little respect to the physical space it is displayed in. This is simply not needed for film. Suppose we would place a second television in the room and we would want it to display the same scene from the first television. We want it to respect the perspective the spectator is in. Then the position of the first television, the second television, and the spectators needs to be known. If we need those positions we might as well make sure these positions match the physical space. Matching the physical and the virtual realm&#8217;s coordinates is an essential ingredient for mixing multiple media in multiple realities. We refer to these developments as Mixed Reality.<br />Since we now have a television which can act as a window to our virtual world we can add our first virtual object. We will add a simple sphere which is hovering in the middle of the room.\n  Ball:\n   position: (1.0, 2.0, 1.0)\n   dimension: (0.2, 0.2, 0.2)\n   orientation: (0, 0, 0)<br />Suppose we would let the ball bounce through the room indefinitely. If this was a physical ball we would be able to see it. However since it is a virtual ball the only way to see it would be through the television. The only moments we would be able to witness the ball through the television would be when the ball would be within the vision line of you (the spectator) and the television. We already know the position of the ball. We know the position of the television. We only don&#8217;t know the position of you. In this scenario we will register the position of your eyes.\n  Spectator:\n   position: (1.0, 1.0, 1.6)\n   dimension: (0.6, 0.3, 1.7)\n   orientation: (0, 0, 0)<br />We can now have enough information to render the ball and display it on the television. A virtual camera will be placed at the exact position of your eyes. It&#8217;s orientation will be perpendicular facing the television and its lens will be shifted so that it exactly matches the frame of the television. You can now see the ball through the television when it moves through the room.</p>\n\n\t<h3>5.1. Overview of data:</h3>\n\n\t<p>root:\n WGS84: (52.1153016, 5.1389998)\n Room: \n  position: (0,0,0)\n  dimension: (2.0, 4.0, 2.0)\n  orientation: (radians? Euler?)\n  matrix: (describes, position, rotation and \n           scale in 9 floats)\n  Chair: \n   position: (1,5, 3.5, 0)   \n   dimension: (0.6, 0.8, 0.9)\n   orientation: (0, 3.14, 0)\n Television:\n  positition: (1.0, 2.0, 0.7)\n  dimension: (0.8, 0.2, 0,45)\n  orientation: (0, -3,0, 0)\n Ball:\n  position: (1.0, 2.0, 1.0)\n  dimension: (0.2, 0.2, 0.2)\n  orientation: (0, 0, 0)\n Spectator:\n position: (1.0, 1.0, 1.6)\n dimension: (0.6, 0.3, 1.7)\n orientation: (0, 0, 0)</p>\n\n\t<h2>6. Acknowledgments</h2>\n\n\t<p>We would like to thank the z25.org Foundation and <span class=\"caps\">MAPLAB</span> of the <span class=\"caps\">HKU</span> University of the Arts for supporting this research. As this research is referring to very common systems and facts we are referring to Wikipedia pages as they are more accessible  in-depth documents than many scientific documents.</p>\n\n\t<h2>7. References</h2>\n\n\t<p>[1] A. Loonstra, \u201cOrchestrating computer systems, a research into a new protocol,\u201d <span class=\"caps\">FOSDEM</span> 2015 conference, Brussels, 01-Feb-2015. [Online]. Available: https://fosdem.org/2015/schedule/event/deviot02/. [Accessed: 08-Apr-2015].</p>\n\n\t<p>[2] \u201cMetric system,\u201d Wikipedia, the free encyclopedia. 15-Jul-2015.</p>\n\n\t<p>[3] \u201cRotation formalisms in three dimensions,\u201d Wikipedia, the free encyclopedia. 07-Jul-2015.</p>\n\n\t<p>[4] \u201cWorld Geodetic System,\u201d Wikipedia, the free encyclopedia. 15-Jun-2015.</p>\n\n\t<p>[5] J. Burbank, <> D. M., and <> W. K., \u201cNetwork Time Protocol Version 4: Protocol and Algorithms Specification.\u201d [Online]. Available: https://tools.ietf.org/html/rfc5905. [Accessed: 15-Jul-2015].</p>", "_status": "wip\n", "coordspace.odt": "_rd_/coordspace", "_title": "Homogenizing Physical and Virtual Coordinate Spaces \n", "_date": "20150715\n"}, "videostreaming_intro": {"_abstract": "In this document, we explore videostreaming technologies using the Gstreamer framework. Since videostreaming is becoming a commodity it is available for anybody to utilize. However videostreaming technology can be challenging as it can be highly complex. In this document we will introduce the elements needed for videostreaming on a network. We will focus our efforts onto setting up live videostreaming between two hosts. \n\n", "img": {"nasa_curiosity.PNG": "_rd_/videostreaming_intro/img"}, "tee-pipeline.png": {"web": "tee-pipeline_web.jpg", "thumb": "tee-pipeline_thumb.jpg", "md5": "todo"}, "_author": "Arnaud Loonstra <arnaud@z25.org>,\nStichting z25.org,\nLeiden University\n", "pads-pipelines.png": {"web": "pads-pipelines_web.jpg", "thumb": "pads-pipelines_thumb.jpg", "md5": "todo"}, "mjpeg-receiver.png": {"web": "mjpeg-receiver_web.jpg", "thumb": "mjpeg-receiver_thumb.jpg", "md5": "todo"}, "test-pipelines.png": {"web": "test-pipelines_web.jpg", "thumb": "test-pipelines_thumb.jpg", "md5": "todo"}, "_body.textile": "\t<h2>Purpose, Context and History</h2>\n\n\t<p>Videostreaming is available in every consumer mobile phone and every home computer. It is used for teleconferencing, live broadcasting, on-demand television, remote gaming, wireless videoprojection, surveillance cameras, remote drone control, etcetera. The first streaming systems stem from 1920 but only since the late &#8217;90s video streaming has become a commodity as technology progressed and standards emerged. </p>\n\n\t<p>There are many out-of-the-box videostreaming software packages available. (e.g. <span class=\"caps\">VLC</span>, Ustream, Justin.tv) These can be very easy to operate and might suit your purpose perfectly. However it is when you run into the limits of these packages that you need to understand what is going on behind the scenes. The purpose of this document is to introduce you to the technology behind the scenes using the Gstreamer framework.</p>\n\n\t<h2>Operating Principles</h2>\n\n\t<p>Streaming technology usually refers to sending large streams of data between systems. Because the data is too big to send in one go it is cut in to smaller packets of data. These packets are then send sequentially. In order to decrease the size of the data it is often compressed. </p>\n\n\t<p>The operating principle of videostreaming is the same. Basically a video is compressed and then send in packets through a transport.  </p>\n\n\t<p>There are two methods of compressing video data. The first is 'Inter-Frame&#8217; based compression. Think of this as saving every image in the video as a <span class=\"caps\">JPEG</span> image. An example compression algorithm that works accordingly is Motion-<span class=\"caps\">JPEG</span>. Other examples are DV and HuffYUV. The second method is 'Intra-Frame&#8217; based compression and uses the the differences in images. If you start with an image the 'Intra-Frame&#8217; based method only tracks the differences in the following frames. Some highly sophisticated algorithms have been developed over the years of which the most used one is H.264. Other examples include Theora, Xvid and Divx. Compression algorithms for video are often referred to as a 'codec&#8217;. </p>\n\n\t<p>To transport the stream of of video data packets there are many possibilities. In TCP/IP networks an <span class=\"caps\">UDP</span> transport is the most simple solution. The <span class=\"caps\">RTP</span> protocol is a transport protocol on top of <span class=\"caps\">UDP</span>. Nowadays <span class=\"caps\">HTTP</span> is also often used as a transport for streaming video. </p>\n\n\t<h2>Strengths and Weaknesses</h2>\n\n\t<p>With digital video technologies many parameters are involved which have a broad range of consequences. We could dive into the discussion of whether IP networks are designed for streaming technologies however we rather focus on the implications of the codecs and the transports on top of IP networks as that&#8217;s the practical situation we will be exposed to. In this document we will be exploring the Motion-<span class=\"caps\">JPEG</span> and H.264 codec on top of <span class=\"caps\">RTP</span> transports.</p>\n\n\t<p>To transport the videostream we already introduced <span class=\"caps\">UDP</span>. As <span class=\"caps\">UDP</span> does not guarantee delivery nor order it&#8217;s only suitable for situations where speed and minimal bandwidth are a top requirement. However usually you do want the right order of packets if the packets do make it across. The <span class=\"caps\">RTP</span> protocol provides this on top of <span class=\"caps\">UDP</span>. Therefore the <span class=\"caps\">RTP</span> protocol is better suited for transporting videostreams. The <span class=\"caps\">HTTP</span> protocol was never designed to do streaming. However as a lot of firewalls block everything except <span class=\"caps\">HTTP</span>, <span class=\"caps\">HTTP</span> is nowadays used for everything thus including videostreaming. </p>\n\n\t<p>When it comes to compressing video the Motion-<span class=\"caps\">JPEG</span> compression is a common 'Inter-Frame&#8217; compression method which simply consists of compressing to <span class=\"caps\">JPEG</span> images. This is very suitable for situations where you need fast encoding and decoding. As it&#8217;s based on single frames Motion-<span class=\"caps\">JPEG</span> is also very suitable to seek through the video. </p>\n\n\t<p>Seeking through a video is much more difficult when the compression is 'Intra-Frame&#8217; based. This method uses the changes in sequential frames. Before finding a frame at a certain position in the video the seek method first needs to find a full frame (keyframe) and from there calculate the differences to the position. The H.264 is a codec based on the differences in frames and therefore less suited for situations where you do a lot of seeking in the videostream. However when it comes to bandwidth the H.264 codec is the clear winner compared to Motion-<span class=\"caps\">JPEG</span>. We&#8217;ll see the differences in bandwidth further on. The H.264 codec was designed for streaming. It provides many parameters to tweak the compression to specific needs. There are too many parameters to manage H.264 but luckily most encoders provide presets. We&#8217;ll be using these presets to do our encoding.</p>\n\n\t<h2>Typicial Applications</h2>\n\n\t<p>The band &#8220;Severe Tire Damage&#8221; performed at Xerox Parc in 1993 while being streamed through the internet. This is one of the first mentioned internet videostreaming events. Videostreaming is used in many domains nowadays of which television is the first that comes to mind. But other examples include streaming your videos on your mobile to your television or your presentation to the video projector. Nowadays a lot of research is done is for remote gaming. The computer you play on is located somewhere in a datacentre while you play it at home on your television. For these applications the latency if of utmost importance. <br />The basic application of videostreaming is sending any picture to one or more receivers. </p>\n\n\t<h2>Surprising Applications</h2>\n\n\t<p><img src=\"img/nasa_curiosity.PNG\" alt=\"\" /><br /><i>Still from the descent of Curiosity Rover. Image credit: NASA/JPL-Caltech/MSSS</i></p>\n\n\t<p>The landing of NASA&#8217;s Curiosity Rover was broadcasted live from Mars. The latency of the videostream was about 14 minutes because of the distance. For the scientists involved this was called the &#8220;7 Minutes of Terror&#8221; as getting from the top of the atmosphere to the surface of Mars was about 7 minutes. So any first signal from Curiosity descent would mean it could be already dead for 7 minutes. </p>\n\n\t<h2>Getting Started</h2>\n\n\t<p>To use the Gstreamer framework it&#8217;s easiest to install it on a Linux system. In this example we are using Ubuntu but the steps should be similar on other platforms. To make sure the framework is installed run the following command in the terminal:</p>\n\n<pre>\nsudo apt-get install gstreamer1.0-tools \\\n  gstreamer1.0-plugins-base \\\n  gstreamer1.0-plugins-good \\\n  gstreamer1.0-plugins-bad \\\n  gstreamer1.0-plugins-ugly\n</pre>\n\n\t<p>To have a basic understanding of the Gstreamer framework you need to think of it as a pipeline. The video data starts at the source and moves to the sink. Meanwhile you can do many things with the videodata. Each chain in the pipeline is called an element. </p>\n\n\t<p><img src=\"basic-pipelines.png\" alt=\"\" /></p>\n\n\t<p>To construct a pipeline we have a very simple command line tool called 'gst-launch&#8217;. The most simple pipeline would be a simple test video display which consists of the following elements:\n\t<ol>\n\t\t<li>videotestsrc: A simple element creating a test image</li>\n\t\t<li>autovideosink: A display element which needs no configuring</li>\n\t</ol></p>\n\n\t<p><img src=\"test-pipelines.png\" alt=\"\" /></p>\n\n\t<p>To create this pipeline run the following command:</p>\n\n<pre>\ngst-launch-1.0 videotestsrc ! autovideosink\n</pre>\n\n\t<p>The 'gst-launch-1.0&#8217; command uses the exclamation mark (!) to link elements to each in order to create a pipeline.</p>\n\n\t<p>In our videostreaming setup between two hosts we already know what we need our pipeline to do. On the sending side we need to:\n\t<ol>\n\t\t<li>acquire the video data</li>\n\t\t<li>compress the video data</li>\n\t\t<li>cut the data into smaller packets</li>\n\t\t<li>send the packets out through a network transport</li>\n\t</ol></p>\n\n\t<p>On the receiving side we than want to:\n\t<ol>\n\t\t<li>receive the packets from the network transport</li>\n\t\t<li>reassemble the packets into video data</li>\n\t\t<li>decompress the video data</li>\n\t\t<li>display the video</li>\n\t</ol></p>\n\n\t<p>To construct this pipeline you first need to find the elements that can do this. In general it is best to lookup the gstreamer plugin documentation to find the elements you need. In the case of a Motion-<span class=\"caps\">JPEG</span> streaming setup using <span class=\"caps\">RTP</span> we need the following elements:\n\t<ol>\n\t\t<li>'videotestsrc&#8217; & 'autovideosink&#8217; to genereate and display an image</li>\n\t\t<li>'jpegenc&#8217; & 'jpegdec&#8217; to encode to and decode from <span class=\"caps\">JPEG</span></li>\n\t\t<li>'rtpjpegpay&#8217; & 'rtpjpegdepay&#8217; to create the <span class=\"caps\">RTP</span> packets</li>\n\t\t<li>'udpsrc&#8217; & 'udpsink&#8217; to transport the <span class=\"caps\">RTP</span> packets using <span class=\"caps\">UDP</span></li>\n\t</ol></p>\n\n\t<p>Run the following two commands for the sender and the receiever:</p>\n\n\t<p>Sender:<br /><pre>\ngst-launch-1.0 videotestsrc ! \\\n jpegenc ! \\\n rtpjpegpay ! \\\n udpsink host=127.0.0.1 port=5200\n</pre></p>\n\n\t<p><img src=\"mjpeg-sender.png\" alt=\"\" /></p>\n\n\t<p>Receiver:<br /><pre>\ngst-launch-1.0 udpsrc port=5200 ! \\\n rtpjpegdepay ! \\\n jpegdec ! \\\n autovideosink\n</pre></p>\n\n\t<p>You&#8217;ll notice that the receiver will quit immediately with an error. This is because we need to tell the 'rtpjpegdepay&#8217; element some information about the data it will receive. This is called a 'capsfilter&#8217; in Gstreamer terms. A capsfilter is placed between the 'pads&#8217; of connecting elements. In Gstreamer events can transfer up and down the pipeline. These events can be used to pause the pipeline for example but it can also be used for exchanging the capabilities.</p>\n\n\t<p><img src=\"pads-pipelines.png\" alt=\"\" /></p>\n\n\t<p>This might look a bit puzzling but just think of it as setting some required parameters the elements in the pipeline need in order to connect to each other:</p>\n\n<pre>\ngst-launch-1.0 udpsrc port=5200 ! \\\n application/x-rtp,\\\n encoding-name=JPEG,payload=26 ! \\\n rtpjpegdepay ! \\\n jpegdec ! \\\n autovideosink\n</pre>\n\n\t<p>Now that we have an example stream working we can replace the 'videotestsrc&#8217; with a webcam. On a Linux system we can use the 'v4l2src&#8217;. We will need a capsfilter to set the webcam&#8217;s resolution.</p>\n\n\t<p><img src=\"mjpeg-receiver.png\" alt=\"\" /></p>\n\n<pre>\ngst-launch-1.0 v4l2src ! \n video/x-raw,width=640,height=480 ! \\\n jpegenc ! \\\n rtpjpegpay ! \\\n udpsink host=127.0.0.1 port=5200\n</pre>\n\n\t<p>This gives us a nice feedback on the latency involved in this stream. The bandwidth used is about 1800 kbit/s. If we now try a default h.264 encoder you will notice difference. </p>\n\n\t<p>Sender:<br /><pre>\ngst-launch-1.0 v4l2src ! \\\n video/x-raw,width=640,height=480 ! \\\n x264enc ! h264parse ! rtph264pay ! \\\n udpsink host=127.0.0.1 port=5000\n</pre></p>\n\n\t<p>Receiver:<br /><pre>\ngst-launch-1.0 udpsrc port=5000 ! \\\n application/x-rtp,\\\n encoding-name=H264,payload=96 ! \\\n rtph264depay ! h264parse ! avdec_h264 ! \\\n autovideosink\n</pre></p>\n\n\t<p>First, it might take very long for the image to show. This is because the stream first needs to receive a full keyframe. Second, your computer might not be able to cope with the encoding and decoding. Third, the latency is much higher (+5s) compared to the Motion-<span class=\"caps\">JPEG</span> pipeline. On the other hand the bandwidth is much lower. About 300 kbit/s. We can tweak the parameters of the x264 encoder (sender) to make it more suitable for live streaming. You can find out which parameters you can set using the 'gst-inspect&#8217; command. Just try the following 'gst-inspect&#8217; command and match the output with the next 'gst-launch&#8217; example. Running 'gst-inspect&#8217; with no arguments will list all available elements.<br /><pre>\ngst-inspect-1.0 x264enc\n</pre></p>\n\n<pre>\ngst-launch-1.0 v4l2src ! \\\n video/x-raw,width=640,height=480 ! \\\n x264enc tune=zerolatency byte-stream=true \\\n bitrate=3000 threads=2 ! \\\n h264parse config-interval=1 ! \\\n rtph264pay ! udpsink host=127.0.0.1 port=5000\n</pre>\n\n\t<p>The bandwidth remains about the same (350 kbit/s) but the latency is much improved as well as the burden on the machine running it.</p>\n\n\t<p>To conclude these example pipelines we still need to know the latencies involved in the pipelines. I will provide a recipe to explore yourself as you now should be able to find out what all these elements do. Basically the recipe constructs a videostreaming setup which is displaying the image with a timestamp on the source computer as well as on a remote system. The difference in the timestamp corresponds with your latency of the pipeline.</p>\n\n\t<p><img src=\"tee-pipeline.png\" alt=\"\" /></p>\n\n\t<p>Sender:<br /><pre>\ngst-launch-1.0 v4l2src ! \\\n video/x-raw,width=640,height=480 ! \\\n timeoverlay ! \\\n tee name=&#34;local&#34; ! \\\n queue ! \\\n autovideosink local. ! \\\n queue ! jpegenc! rtpjpegpay ! \\\n udpsink host=127.0.0.1 port= 5000\n</pre></p>\n\n\t<p>Receiver:<br /><pre>\ngst-launch-1.0 udpsrc port=5000 ! \\\n application/x-rtp,\\\n encoding-name=JPEG,payload=26 ! \\\n rtpjpegdepay ! jpegdec ! autovideosink\n</pre></p>\n\n\t<h2>Final Thoughts</h2>\n\n\t<p>This document introduced you into the technologies involved in video streaming across networks. The Gstreamer framework was introduced and you should now be confident to experiment with different pipeline setups. </p>\n\n\t<p>Not everything was explained as video streaming technologies is an engaging subject. As this paper only covered videostreaming no audio was involved. However in many videostreaming situations you do want audio. Therefore you need to understand that in those cases a data stream needs to contain both video and audio. The process of combining and splitting the audio and video data is called 'multiplexing&#8217;. The elements doing this process are called a 'muxer&#8217; and a 'demuxer&#8217;. </p>\n\n\t<p>If you want to use the Gstreamer Framework in your own program I urge you to read the official Gstreamer tutorials. The process is exactly the same as explained in this document but provides many more options. As long as you understand the basic principles and commands introduced in this document you should be able to code any pipeline.</p>\n\n\t<h2>References</h2>\n\n\t<ol>\n\t\t<li><a href=\"http://gstreamer.freedesktop.org/documentation/plugins.html\">Overview of all Gstreamer plugins</a>, http://gstreamer.freedesktop.org/documentation/plugins.html, Retrieved 2014-06-04</li>\n\t\t<li><a href=\"http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/index.html\">Gstreamer Application Development Manual</a>, http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/index.html, Retrieved 2014-06-04</li>\n\t</ol>", "basic-pipelines.png": {"web": "basic-pipelines_web.jpg", "thumb": "basic-pipelines_thumb.jpg", "md5": "todo"}, "_status": "completed\n", "mjpeg-sender.png": {"web": "mjpeg-sender_web.jpg", "thumb": "mjpeg-sender_thumb.jpg", "md5": "todo"}, "_title": "Videostreaming with Gstreamer\n", "_date": "20140605\n"}, "dune2threads": {"_abstract": "Multithreading is a potential programmers nightmare. Key in preventing problems while programming multithreaded programs is understanding the source of where problems could arise. In this article the behavior of multithreaded programs is suggested using a metaphor based on the strategy game Dune 2 from the 90's.\n", "img": {"img2.jpg": "_rd_/dune2threads/img"}, "_body.textile": "\t<h2>Introduction</h2>\n\n\t<p>Multithreading is the method for computer programs to utilize all processors in a multi processor system. Nowadays it&#8217;s hard to find a system which only consists of a single core. The increase of processor speeds have come to a halt and the remedies around this are multiple processors. In the article 'The Problem with Threads ' [1] by Edward A. Lee of the University of California at Berkeley it is argued that the ongoing trend towards parallelism is a too big steps from the sequential paradigm of programming computers. \u201cAlthough threads seem to be a small step from sequential computation, in fact, they represent a huge step. They discard the most essential and appealing properties of sequential computation: understandability, predictability, and determinism. :understanding of threads is essential.\u201d</p>\n\n\t<p>Since Lee is saying we need to understand in order to control we should first understand the problem. Nobody will understand a solution if not knowing what the problem is. I propose to explain the problem of multithreading by using a game. This game will be based on Dune II. For this proposal I assume the reader is familiar with the game Dune II [2]</p>\n\n\t<h2>Sequential computing</h2>\n\n\t<p>The most used and understood paradigm is sequential computing. The game would start with a single harvester, a refinery and a spice field. The harvester collects the spice and delivers it at the refinery.</p>\n\n\t<p><img src=\"img1.jpg\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>In this metaphor the harvester is the thread, the refinery the output and the spice field is the memory segment assigned to the harvester. The harvester is instructed to collect spice until it is full and deliver it to the refinery. The start of the game illustrates sequential computing.</p>\n\n\t<h2>Multithreading</h2>\n\n\t<p>Now imagine if we had a second harvester. </p>\n\n\t<p><img src=\"img/img2.jpg\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>We would be collecting spice with twice the speed. In a physical world situation we would not imagine the two harvesters wanting to collect spice on the same spot. The drivers would see if the spot is occupied. However these harvesters are fully automated ones and thus are not equipped with eyes nor common sense.</p>\n\n\t<p><img src=\"img3.jpg\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>If the two harvesters would be assigned to the same spice field they could run into the situation that they crash into each other when harvesting the same spot. Since the spice, in our game, is highly explosive we would have no spice at all and lose both harvesters.</p>\n\n\t<p>We need to build a protection system in order to prevent a situation like this. A very simple one would be to assign specific areas of the spice field to each harvester.</p>\n\n\t<p><img src=\"img4.jpg\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>This way the two harvesters would never be in each others way and both harvesters can operate autonomous. </p>\n\n\t<p>There are many more methods in order to prevent the harvesters from crashing. For the illustration of this metaphor I will leave these for the reader to explore.</p>\n\n\t<h2>Mutex</h2>\n\n\t<p>Once a harvester is finished, it is programmed to deliver its spice cargo to the refinery. </p>\n\n\t<p><img src=\"img5.png\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>We can imagine a situation where two harvesters are both returning to the refinery. However only one can be in the refinery at a time.</p>\n\n\t<p><img src=\"img6.png\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>Since the harvesters are fully automated the second harvester would just drive into the refinery and crash into the harvester already there. Since the spice is highly explosive we would not only lose the harvesters but also the complete refinery. </p>\n\n\t<p>In order to prevent this disastrous situation we would need some locking mechanism which will prevent the second harvester from driving into the refinery. The harvesters would need to be equipped with some method to request if the refinery is available. We are going to equip the refinery with a mutex lock. The harvesters will request availability of this mutex. If the mutex is available the harvester will have access to the mutex and needs to lock the mutex so others can&#8217;t access it. This way the harvesters will only enter the refinery if it could lock the refinery. If not it needs to wait before it can. </p>\n\n\t<p>We now have a safe way of operating multiple harvesters but our unstoppable urge to get more spice has made us order more harvesters. They are doing their job fine but we can now notice something else happening:</p>\n\n\t<p><img src=\"img7.png\" style=\"width:356;\" alt=\"\" /></p>\n\n\t<p>We have 3 harvesters waiting to enter the refinery. Of course this is not efficient since these harvesters are doing nothing while waiting. Our demand for spice forces us to make this more efficient. </p>\n\n\t<p>It&#8217;s obvious we need another refinery. Which will bring more methods to the harvesters for finding entrance to a refinery. The harvesters are becoming more expensive to operate because of all these methods. We&#8217;ll introduce the semaphore concept in this game in order do determine refinery availability. Continuing with the game we will also introduce the 'Carryall&#8217;. This airplane will  replace the methods of the mutex lock on the refinery and will organize which harvester will go to which refinery. It would also be possible to introduce pipes and sockets to transfer the spice around. Introducing these concepts in order to illustrate multithreading methods used in modern implementations like Erlang will introduce new assets into the game which are not in the original game. </p>\n\n\t<h2>Conclusion</h2>\n\n\t<p>The article by Edward A. Lee argues that parallelism is too complicated for programmers to understand. In my research into understanding this I first ran into trying to understand the problem. By using a single context for illustrating the problems a more fundamental understanding of what&#8217;s happening and how it&#8217;s circumvented is more accessible. The Dune II metaphor I propose would be a single context to explain the problems and illustrate methods and solutions in an accessible way. </p>\n\n\t<h2>References:</h2>\n\n\t<ol>\n\t\t<li>Edward A. Lee, The Problem with Threads, <span class=\"caps\">EECS</span> Department, University of California, Berkeley, 2006, Jan, UCB/EECS-2006-1, <span class=\"caps\">IEEE</span> Computer 39(5):33-42, May 2006</li>\n\t\t<li>http://en.wikipedia.org/wiki/Dune_II</li>\n\t</ol>", "_author": "Arnaud Loonstra <arnaud@z25.org>, Stichting z25.org\n", "img4.jpg": "_rd_/dune2threads", "img5.png": {"web": "img5_web.jpg", "thumb": "img5_thumb.jpg", "md5": "todo"}, "img6.png": {"web": "img6_web.jpg", "thumb": "img6_thumb.jpg", "md5": "todo"}, "img7.png": {"web": "img7_web.jpg", "thumb": "img7_thumb.jpg", "md5": "todo"}, "img1.jpg": "_rd_/dune2threads", "_status": "completed", "img3.jpg": "_rd_/dune2threads", "_title": "Multithreaded race conditions explained using Dune II\n", "_date": "20131017\n"}, "inputanalyser": {"ia-test.cmo": "_rd_/inputanalyser", "body.md": "<h1>Live Sound Input Analyser for Virtools 2.x</h1>\n<p>For the project <a href=\"/#project?takt\">TAKT</a> we needed a plugin for the program Virtools 2.1 to analyse sound input 'real time'. We dived into the Virtools SDK and main coder of Knip 'n Plak wreck and myself coded the plugin. Now the <a href=\"/#project?takt\">TAKT</a> project has finished and performed we decided to release the plugin and code under the GNU Lesser Public License.</p>\n<p><img alt=\"http://www.k-n-p.org\" src=\"_rd_/inputanalyser/knp.png\" /></p>\n<p><strong>UPDATE</strong>: Version 2 supports stereo sound. We know some people have been adding this already. We've included this feature in this new release.</p>\n<p>Thanks to Jon Lee we now have the plugin running under Dev2.5. It's compiled against DirectX 9. We haven't tested it ourselves so please provide us with feedback.<br />\n</p>\n<h2>Downloads</h2>\n<p><strong>NEW!!!</strong> stereo support</p>\n<ul>\n<li><a href=\"ia:InputAnalyser-v2.0.zip\">InputAnalyser version 2.0</a></li>\n<li><a href=\"ia:ia2-test.cmo\">InputAnalyser version 2.0 basic test cmo</a></li>\n<li>\n<p><a href=\"ia:InputAnalyser-v2.0-src.zip\">InputAnalyser version 2.0 source code</a></p>\n</li>\n<li>\n<p><a href=\"ia:InputAnalyser-v0.6.zip\">InputAnalyser version 0.6</a></p>\n</li>\n<li><a href=\"ia:InputAnalyser-v0.6_Dev2.5.zip\">InputAnalyser version 0.6 - Dev 2.5</a></li>\n<li><a href=\"ia:ia-test.cmo\">InputAnalyser version 0.6 basic test cmo</a></li>\n<li><a href=\"ia:InputAnalyser-v0.6-src.zip\">InputAnalyser version 0.6 source code</a></li>\n</ul>\n<p>Please report any problems since this plugin has not been tested thoroughly</p>\n<h2>Virtools 4.0/5.0</h2>\n<p>Ben Cole has provided the dll's for version 4.0 and 5.0:</p>\n<ul>\n<li><a href=\"ia:inputanalyservt40.dll\">InputAnalyser for Virtools 4.0</a></li>\n<li>\n<p><a href=\"ia:inputanalyservt50.dll\">InputAnalyser for Virtools 5.0</a></p>\n<p>The error was caused because the variable \"left\" was a BOOL which seems to be understood differently in the newer version of Visual Studio. I simply changed it to a float or integer to solve the problem.</p>\n<p>Attached are the dll's for Virtools 4.0 and Virtools 5.0, I have tested and confirmed that the 4.0 one is working beautifully. I will test the 5.0 out tomorrow, but I do not forsee any problems. Feel free to post them on your site and do whatever you want with them.</p>\n<p>-Ben Cole</p>\n</li>\n</ul>\n<h2>System requirements</h2>\n<ul>\n<li>DirectX 8.1 or higher</li>\n<li>Soundcard installed.</li>\n<li>Virtools 2.x (Virtools 2.5 has not been tested by us)</li>\n</ul>\n<p><em>We've found problems while using Windows '98. Best results were with Windows 2000 which is the target platform.</em></p>\n<h2>Installation notes</h2>\n<p>Just copy the DLL into your 'managers' directory. When you start Virtools you will find a new section Building Blocks - Sound - InputAnalyser in which you'll find three Building Blocks:</p>\n<ul>\n<li><strong>IAGetFreqBB:</strong> This building block will output FFT frequency analysis. The freqeuncy range is 0 - 22KHz. This is divided into 256. You can add input parameters (integers) so you can select a range of frequencies. (i.e. integer 1 and 3 will output a range of about 87 Hz to 261 Hz).</li>\n</ul>\n<p><img alt=\"\" src=\"ia:iaBB-action2.gif\" /></p>\n<p>From version 2.0 the plugin supports stereo sound. You can set which side to output.</p>\n<ul>\n<li>\n<p><strong>IAGetMaxFreqBB:</strong> This Building block will output the array number (number between 0-255) of the frequency most found in the sound. From this number you can calculate the frequency range. (the output number times 87)</p>\n</li>\n<li>\n<p><strong>IAGetVolBB:</strong> This building block will output the volume of the sound. The output is an integer</p>\n</li>\n</ul>\n<h2>License &amp; copyrights</h2>\n<p>This plugin is copyright 2003/2004 sphaero.org/KnP. All rights reserved. The plugin has been released under the Gnu Lesser Public License for others to enjoy.</p>\n<p>We would like to know if people are using this plugin for any purpose. If you make modifications or additions to this plugin please let us know so we might include it into future releases of the plugin.</p>\n<p>Problems or questions please contact us by email</p>", "ia2-test.cmo": "_rd_/inputanalyser", "img": {"iabb-action2.gif": "_rd_/inputanalyser/img"}, "_description": "Live sound input analysis plugin for Virtools\n", "inputanalyser-v2.0-src.zip": "_rd_/inputanalyser", "inputanalyser-v0.6.zip": "_rd_/inputanalyser", "inputanalyser-v0.6-src.zip": "_rd_/inputanalyser", "_title": "Input Analyser\n", "inputanalyser-v0.6_dev2.5.zip": "_rd_/inputanalyser", "inputanalyservt40.dll": "_rd_/inputanalyser", "inputanalyser-v2.0.zip": "_rd_/inputanalyser", "inputanalyservt50.dll": "_rd_/inputanalyser", "knp.PNG": "_rd_/inputanalyser", "_date": "20030505\n"}, "howto_archive": {"body.md": "<h1>How-to</h1>\n<p><em>A how-to is an informal, often short, description of how to accomplish a specific task. A how-to is usually meant to help non-experts, may leave out details that are only important to experts, and may also be greatly simplified from an overall discussion of the topic.</em> (source: Wikipedia)</p>\n<p>Over the years I have written quite a few of these 'How-tos'. I keep a list here of some of the published documents. They usually target technical topics.</p>\n<h2>Egroupware Server with an LDAP backend</h2>\n<p>This article explains how to setup an Egroupware server with an LDAP backend on Debian Etch. The article is published on <a href=\"http://www.debian-administration.org/articles/394\">Debian Administration</a>.</p>\n<h2>Debian Preseeding</h2>\n<p>This article explains how to do automated installs of Debian (Sarge). The article is published on <a href=\"http://www.debian-administration.org/articles/377\">Debian Administration</a>.</p>\n<h2>Multiple uplinks</h2>\n<p>This atricle explains how to setup routing for multiple uplinks using Debian Linux. The article is published on <a href=\"https://www.debian-administration.org/article/377/Routing_for_multiple_uplinks\">Debian Administration</a>.</p>\n<h2>W2K Samba deploy HOWTO</h2>\n<p>A 'HOWTO' about deploying Windows 2000 in a Samba controlled network. This document mostly focus on setting up and rolling out Windows 2000.</p>\n<p>While using Samba as main servers in mostly Windows networks I always found it hard to find information about doing certain things in Windows. I'm no MCSE-er and do not ambition to be one ever. It's clear that the information in this document is very accessible for a MCSE-er but for others I found it hard to get all information together and more important FREE. This document came together after many implementations of Samba servers and looking up Usenet archives and other resourceful websites. It has been referenced on the main Samba website. However by now the information is quite outdated.</p>\n<p>The document is written in English:</p>\n<ul>\n<li><a href=\"w2k-samba-deploy-howto.pdf\">w2k-samba-deploy-howto.pdf</a></li>\n</ul>", "_description": "Archive of published How-tos\n", "w2k-samba-deploy-howto.pdf": "_rd_/howto_archive", "_date": "20050303\n", "_title": "How-tos\n", "docs_w2k-samba-deploy-howto.sxw": "_rd_/howto_archive"}, "stereofrustum_bb": {"stereofrustum-v1.02b.zip": "_rd_/stereofrustum_bb", "body.md": "<h1>Virtools Stereo Frustum BB</h1>\n<p>This plugin will enable you to calculate a correct projection frustum for usuage in stereographics projection. It calculates an off-axis projection frustum from the Eyedistance and Focal distance in 3D. The plugin has been created for use in a passive stereoscopic setup. It's been created for a research about historical stereophotographics and stereographics for use in theatre performances. This research was initiated by <a href=\"http://www.lange-poten.nl\">Stichting Lange Poten</a>. The plugin was written by <a href=\"http://www.k-n-p.org\">Knip 'n Plak</a> coder Wreck and myself.\n<img alt=\"KnP Logo\" src=\"/_rd_/stereofrustum_bb/knp.png\" /></p>\n<p>Most of the techniques are based on documents from <a href=\"http://www.paulbourke.net\">Paul Bourke</a>.</p>\n<p>Note that <a href=\"https://en.wikipedia.org/wiki/Virtools\">Virtools</a> doesn't exist anymore.\nThis page is just kept for historic purposes.</p>\n<h2>Downloads</h2>\n<ul>\n<li><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.02b.zip\">StereoFrustum version 1.02b</a></li>\n<li><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.01b.zip\">StereoFrustum version 1.01b</a></li>\n<li>\n<p><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.0b.zip\">StereoFrustum version 1.0b</a></p>\n</li>\n<li>\n<p><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.02b.src.zip\">StereoFrustum version 1.02b source code</a></p>\n</li>\n<li><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.01b.src.zip\">StereoFrustum version 1.01b source code</a></li>\n<li><a href=\"/_rd_/stereofrustum_bb/stereofrustum-v1.0b.src.zip\">StereoFrustum version 1.0b source code</a></li>\n</ul>\n<p>Thanks to Gilles Pinault who provided us with a Virtools Dev 2.5 version.</p>\n<ul>\n<li><a href=\"/_rd_/stereofrustum_bb/stereofrustum25.zip\">StereoFrustum25.zip</a></li>\n</ul>\n<h2>Installation Notes</h2>\n<p>Just copy the DLL into your 'Building Blocks' directory. When you start Virtools you will find FrustumBB in section Cameras/FX:</p>\n<p><img alt=\"Virtools Screenshot\" src=\"/_rd_/stereofrustum_bb/img/frustumbb.gif\" /></p>\n<p>This is the StereoFrustum Building Block in which you can set the following settings:</p>\n<ul>\n<li>LeftEyePosition(boolean): should the left(true) or right(false) posistion be calculated</li>\n<li>Eyedistance: The distance from 1 eye to the eyecenter. When using value 0 a workable distance will be calculated.</li>\n<li>Focal: The distance from the camera to the focal point.</li>\n</ul>\n<p>The Building Block outputs the eyedistance. This might be useful for parenting the camera to the eyecenter.</p>\n<h2>License &amp; copyrights</h2>\n<p>This plugin is copyright 2004 sphaero.org/KnP/Lange Poten. All rights reserved. The plugin has been released under the Gnu Lesser Public License for others to enjoy.</p>\n<p>We would like to know if people are using this plugin for any purpose. If you make modifications or additions to this plugin you are obliged to provide the source to us to include it into future releases of the plugin.</p>\n<p>Problems or questions please contact us by email</p>\n<h2>Changelog</h2>\n<pre><code># v1.02b 16-01-04\n- fix incorrect eyedistance usuage\n\n# v1.01b - 13-01-04\n- fix incorrect correction after editing BB\n\n# v1.0b - 09-01-04\n- release date!\n</code></pre>", "stereofrustum-v1.02b.src.zip": "_rd_/stereofrustum_bb", "img": {"frustumbb.gif": "_rd_/stereofrustum_bb/img"}, "_description": "This plugin will enable you to calculate a correct projection frustum for usuage in stereographics projection.\n", "stereofrustum-v1.0b.zip": "_rd_/stereofrustum_bb", "knp.png": {"web": "knp_web.jpg", "thumb": "knp_thumb.jpg", "md5": "todo"}, "stereofrustum-v1.01b.zip": "_rd_/stereofrustum_bb", "stereofrustum-v1.01b.src.zip": "_rd_/stereofrustum_bb", "stereofrustum-v1.0b.src.zip": "_rd_/stereofrustum_bb", "_title": "Virtools StereoFrustum Building Block\n", "_date": "20040506\n", "stereofrustum25.zip": "_rd_/stereofrustum_bb"}, "freaklab": {"_abstract": "Every new technology needs a tryout in practice. Apart from user tests it is often difficult to prepare a new and immature technology for real use cases. To pave the way for a new technology to practical use we have experimented with artists and developers playing with the technology in a controlled lab sessions (Freaklabs). Although this proves difficult to organize the results are invaluable to steer further development of the technology. In this whitepaper we evaluate three Freaklab sessions for the ZOCP technology.\n", "img": {"fl3_presentations.jpg": "_rd_/freaklab/img", "fl2_context.jpg": "_rd_/freaklab/img", "fl3_setup.jpg": "_rd_/freaklab/img", "fl2_discuss.jpg": "_rd_/freaklab/img"}, "freaklab.odt": "_rd_/freaklab", "_tags": "zocp, R&D, lab, workflow\n", "_author": "Arnaud Loonstra, arnaud@z25.org, Stichting z25.org\n", "freaklab.pdf": "_rd_/freaklab", "_body.textile": "\t<h1>Introduction</h1>\n\n\t<p>The <span class=\"caps\">ZOCP</span> technology is a new computer technology which enables artists to easily connect different applications to each other and so create an orchestration of interconnected applications. This is accomplished by a classical programming framework combined with a visual editor to control the interconnections. <br />The <span class=\"caps\">ZOCP</span> technology has emerged from the need to have a more flexible and distributed setup for orchestrating computer technology<sup class=\"footnote\"><a href=\"#fnc7e609df-ddc8-4353-9b37-e7b16e17bf72\">1</a></sup>. This has resulted in a prototype build with ZeroMQ and Python. The technology was developed to solve the problem of centralized control and processing and manual configured setups. However to steer future development we needed to make sure that:\n\t<ol>\n\t\t<li>the problem is indeed solved by the new technology </li>\n\t\t<li>new problems introduced by the new technology are charted</li>\n\t\t<li>users can adopt the technology easily</li>\n\t\t<li>the new technology is solid and future proof</li>\n\t</ol><br />We&#8217;ve found that in the early stages of design and development a very direct communication between the users of the technology and its developers is beneficial. This document describes three sessions which were organized to give the new technology(<span class=\"caps\">ZOCP</span>) a testbed with its users and developers involved.</p>\n\n\t<h1>Methods</h1>\n\n\t<p>We have hypothesized that specific tests or user tests of the technology is not enough to assure optimal development. Specific tests are usually constructed from a technological perspective, however in a young technology even the design of the technology might need testing. The technology perspective towards a solution can be very different from a user&#8217;s perspective. Therefore we think that the best approach to test user acceptance of a technology and evaluate its design is to create a situation where developers and users work together. As the <span class=\"caps\">ZOCP</span> technology is aimed at being used in an artistic context we have organized sessions (Freaklabs) in which artists and developers together play with setups using the new technology. <br />Apart from creating a playground for the technology these Freaklabs are without specific goals. By doing so a setting is created for all participants to objectively perceive the technology. In the Freaklab it is, for example, very valuable to fail while trying something. There is much more to learn from a failed attempt than from a successful one.<br />This document is written as a reflection on the first three organized labs. All Freaklabs were held at the Media and Performance Lab (<span class=\"caps\">MAPLAB</span>) of the <span class=\"caps\">HKU</span> University of the Arts Utrecht, The Netherlands. The first Freaklab was held at June 4 until June 6 2014. This first session was aimed at introducing the prototype to the artists and give the prototype an initial stress test. The second Freaklab was held from October 7 until October 10 2014 and was aimed at using the prototype to orchestrate a mixed reality setup. The third Freaklab was held from May 23 until May 24. This session was aimed at controlling multiple videoplayers and test whether the <span class=\"caps\">API</span> was acceptable to artists.</p>\n\n\t<h1>Results</h1>\n\n\t<p>The first lab gave the <span class=\"caps\">ZOCP</span> prototype an initial stress test while being used on Linux, <span class=\"caps\">OSX</span> and Windows platforms. This lab was especially useful to explain <span class=\"caps\">ZOCP</span> concepts and usage to artists. This proved to be quite challenging as the translation from a conceptual model to the users mental model is a very delicate subject<sup class=\"footnote\"><a href=\"#fnca6b7b23-f864-4aef-a62e-cabce76f3d99\">2</a></sup>[3][4]. This can be best explained in terms of the difference between the familiar sequential programming paradigm to an asynchronous paradigm. The best approach seemed to be providing very simple use cases and discussing the problems of specific solutions artists came up with. Combining this with a solid theoretical conceptual model of the technology enabled the artists to mentally understand how the technology works. Even days after the Freaklab there was still discussion about how to utilize the <span class=\"caps\">ZOCP</span> technology which illustrates the process of forming a mental model from the taught concepts.<br /><img src=\"img/fl2_discuss.jpg\" alt=\"\" /><br />As practice makes perfect the second lab was aimed at continuing to use the <span class=\"caps\">ZOCP</span> technology. Often a new technology does not serve as a drop-in replacement for an existing technology. In our case a lot of systems are using the <span class=\"caps\">OSC</span> protocol. The second Freaklab was therefore trying to combine the <span class=\"caps\">ZOCP</span> technology with existing <span class=\"caps\">OSC</span> applications. In addition the lab session was looking into combining a physical realm with a virtual realm and introducing interaction. This was done by trying to match projected virtual spaces with the physical space and using legacy applications to provide interaction. <br />The second lab showed that matching different realities is a very precise job. When no system provides a clear structure for describing a coordinate space you easily end up calculating a conversion for every space each time. This has resulted in a new research about the coordinate models for different spaces. <br />Blender was used as a tool to operate and control all the virtual realities. The <span class=\"caps\">ZOCP</span> technology was therefore integrated in Blender. This enabled the artists to utilize <span class=\"caps\">ZOCP</span> much better as the Blender tool is closer to an artistic practice than programming in a text editor. <br />As this Freaklab was more focused on creating a setup with <span class=\"caps\">ZOCP</span> as well as legacy applications, <span class=\"caps\">ZOCP</span> received less feedback. However the general feedback from the artists proved invaluable for new developments of <span class=\"caps\">ZOCP</span>, hence the new research into mapping coordinate models. The session also showed the different practices of legacy application and newer <span class=\"caps\">ZOCP</span> based applications. In general the setups create chains of program communications which require a lot of management. This management is exactly what <span class=\"caps\">ZOCP</span> aims to ease as legacy (<span class=\"caps\">OSC</span>) applications do not have such mechanisms. <br />The artistic context which was created in the lab consisted of a interactive story called &#8220;Sea of Glass&#8221;. The story was about a fictional character with a porcelain heart. This story was experienced by interacting a moving head with a pico projector attached to it. The moving head was able to be controlled by using a Wii controller or a tablet and acted as a search light in a panoramic sea environment. It is as if the participant is standing in a lighthouse searching for objects/memories through which the story unfolds. (Illustration 2)<br /><img src=\"img/fl2_context.jpg\" alt=\"\" /><br />The third Freaklab was aimed at using the new signals implementation of ZOCP<sup class=\"footnote\"><a href=\"#fnf29f2e46-3659-4b6b-9e34-2c76c616d711\">5</a></sup>. Signals should enable the artists easier exchange of data between nodes of <span class=\"caps\">ZOCP</span>. This lab session was preceded by a <span class=\"caps\">ZOCP</span> workshop on the 4th of May. The workshop was done to introduce new artists and developers to <span class=\"caps\">ZOCP</span> and to refresh the knowledge of those already familiar with <span class=\"caps\">ZOCP</span>. This workshop proved to be very important to practice teaching the concepts of <span class=\"caps\">ZOCP</span> but also to feed discussion about the concepts. These discussions were very fertile for everybody&#8217;s mental model of <span class=\"caps\">ZOCP</span>. We have noticed this is a repeating subject when introducing the programming model of <span class=\"caps\">ZOCP</span>. <br />After the workshop developers and artists started making simple applications using ZOCP<sup class=\"footnote\"><a href=\"#fn19f9fea3-2f3a-4df3-ab2e-75cbb06997a8\">6</a></sup>. The 3rd session was also fed with an artistic context based on the play &#8220;No Exit&#8221; by Sartre<sup class=\"footnote\"><a href=\"#fnda85b330-6b2b-42a5-9a43-aa0dd631a87a\">7</a></sup> provided by Wijnand Veneberg and Erico Becker. A remediation of the play as a modern interpretation of the underground was created. Participants are immersed in the play in a setting in which six people sit in a circle with a vertical screen in front of each of them. The screens act as gateways to the underworld so the participants gaze into each other&#8217;s underworld. All of them have a room on their screen, of which three are represented with the three characters. In this way the participants become the characters. The other three screens contained visual animated representations with background information of the three characters.<br /><img src=\"fl3_setup.jpg\" alt=\"\" /><br />Initially a physical setup of 6 screens was build using the available hardware. These 6 screen were all controlled by Raspberry Pi machines running a simple <span class=\"caps\">ZOCP</span> controlled videoplayer. <br />Before playing with the <span class=\"caps\">ZOCP</span> apps commenced some developers presented new apps which were developed after the workshop. This was also an introduction to <span class=\"caps\">ZOCP</span> for artists that were new to <span class=\"caps\">ZOCP</span>.<br /><img src=\"img/fl3_presentations.jpg\" alt=\"\" /><br />After these presentations the artists started playing with constructs of <span class=\"caps\">ZOCP</span> apps to trigger different actions. Drumcomputers were used to control videos on the screen. Lights were controlled using <span class=\"caps\">DMX</span> and <span class=\"caps\">ZOCP</span>. Midi controllers were used to trigger events. <br />The third Freaklab was the first time artists were able to fully create setups using <span class=\"caps\">ZOCP</span> without having to develop specific apps. This resulted in discussions about how to create setups using <span class=\"caps\">ZOCP</span> apps which is exactly the discussion we wanted for feedback of <span class=\"caps\">ZOCP</span> development. For example a discussion emerged about how to create inputs for the <span class=\"caps\">ZOCP</span> apps in the form of event triggers and whether certain logic should reside at the output end or at the input end. <br />The role of the node editor became very apparent as a way to connect different nodes of different people and keep an overview of what was happening. This also showed a bias towards nodal logic. Some artists were thinking about <span class=\"caps\">ZOCP</span> in terms of programming logic by visual nodes (ie. PureData, Max/<span class=\"caps\">MSP</span>, <span class=\"caps\">VVVV</span>) as opposed to orchestrating multiple programs running on computers. <br />The limitations of the current prototype were also shown by <span class=\"caps\">ZOCP</span> apps which were developed to workaround certain missing features. For example when two <span class=\"caps\">ZOCP</span> nodes exchange incompatible data a translation needs te be made. A <span class=\"caps\">ZOCP</span> app was developed to provide this feature however in the design of <span class=\"caps\">ZOCP</span> this is provided by applying logic on the signal subscriptions. This is currently not implemented yet hence the workaround provided by a <span class=\"caps\">ZOCP</span> app. <br />The <span class=\"caps\">API</span> of the prototype was also discussed as being too cumbersome. Users don&#8217;t want to deal with the internals of <span class=\"caps\">ZOCP</span> and the <span class=\"caps\">API</span> should provide for that. In the current prototype one needs to access data by pointing into a nested datastructure. A new incarnation of <span class=\"caps\">ZOCP</span> should hide the internals and let users only deal with their own data.</p>\n\n\t<h1>Conclusion</h1>\n\n\t<p>The Freaklabs have proven to be very useful for steering development and testing user adoption. By bringing development and usage close to each other they influence each other naturally. Users need to develop a mental model based on the concepts provided by the new technology. Developers need to understand how these mental models are formed by the user so they can adept the technology to cater for this optimally. By letting developers and users discuss directly they seem to relate to each other better. The developer starts talking in terms of the user and the user starts talking in terms of the developer. <br />As a developers needs to create or enhance something he/she needs to zoom into a specific parts of the process of a program. This needs dedication and concentration. For a user this looks like a very introvert process and can seem intimidating. Hours can go by without any visible progress. In the Freaklabs we have witnessed this often, especially in the early days of the prototype. In general one would want to avoid these situations by making sure some functionality exists before commencing a Freaklab. On the other hand it is sometimes unavoidable. The Freaklabs need to be organized around this. A certain balance needs to be found.<br />What proved very productive is to not have a specific goal. This enabled everybody to have a saying in the process. There is a delicate equilibrium of trying out technical aspects of <span class=\"caps\">ZOCP</span> and being able to play with the technology without being limited by technical aspects. By just providing a playfield with some artistic context to support the fun of playing we&#8217;ve found the emerged discussions to be very helpful for all parties. These discussions are the essence of the Freaklabs as technology only has a value in the context of its users. To develop for this the users and developers need to talk on a same level.<br />Every new technology requires a new mental effort of its adopting users. We&#8217;ve found to repeat the concepts of the technology and to discuss them openly supports users as they form a workable mental model. This process also contributes to developers understanding how users can adopt the technology.<br />Having users play with the technology and have developers join seems a very natural way to acquire feedback on the technology and to estimate its state. However we&#8217;ve found that it takes an effort to facilitate this process as users and developers often talk a very different language and have very different practices. The artistic context is able to provide a common ground for discussion in any terms. The Freaklabs we have organized have been very productive however they are also a luxury. A Freaklab requires effort and time from both users and developers without having a goal other than discover the new technology.</p>\n\n\t<h1>References</h1>\n\n\t<ul>\n\t\t<li>[1] A. Loonstra, &#8220;Orchestrating computer systems, a research into a new protocol&#8221;, <span class=\"caps\">FOSDEM</span> 2015 conference, Brussels, 01-Feb-2015. [Online]. Available: https://fosdem.org/2015/schedule/event/deviot02/. [Accessed: 08-Apr-2015].</li>\n\t\t<li>[2] I. M. Greca and M. A. Moreira, &#8220;Mental models, conceptual models, and modelling&#8221;, Int. J. Sci. Educ., vol. 22, no. 1, pp. 1\u201311, Jan. 2000.</li>\n\t\t<li>[3] A. Robins, J. Rountree, and N. Rountree, &#8220;Learning and Teaching Programming: A Review and Discussion&#8221;, Comput. Sci. Educ., vol. 13, no. 2, pp. 137\u2013172, Jun. 2003.</li>\n\t\t<li>[4] S. Bhuiyan, J. E. Greer, and G. I. McCalla, &#8220;Learning recursion through the use of a mental model-based programming environment&#8221;, in Intelligent Tutoring Systems, C. Frasson, G. Gauthier, and G. I. McCalla, Eds. Springer Berlin Heidelberg, 1992, pp. 50\u201357.</li>\n\t\t<li>[5] A. Hoeben, &#8220;Implementation of Signals&#8221;, GitHub. [Online]. Available: https://github.com/z25/pyZOCP. [Accessed: 01-Jun-2015].</li>\n\t\t<li>[6] &#8220;Collection of <span class=\"caps\">ZOCP</span> applications&#8221;, GitHub. [Online]. Available: https://github.com/z25/ZOCPApps. [Accessed: 01-Jun-2015].</li>\n\t\t<li>[7] GradeSaver, &#8220;No Exit Study Guide | GradeSaver&#8221;. [Online]. Available: http://www.gradesaver.com/no-exit. [Accessed: 28-May-2015].</li>\n\t</ul>\n\n\t<h1>Acknowledgements</h1>\n\n\t<p>We thank the <span class=\"caps\">MAPLAB</span>, z25.org Foundation and all participants for enabling the Freaklab sessions.<br />This paper is published online: <a href=\"http://www.z25.org/?page=randd\">http://www.z25.org/?page=randd</a></p>", "_status": "completed\n", "_title": "Freaklabs: Joint Artists and Developers Technology Design and Evaluation\n", "_date": "20150601\n"}, "virtueeltheatermaker": {"AanDeSlagAlsVirtueelTheatermaker_Loonstra_2005.pdf": "_rd_/virtueeltheatermaker", "body.md": "<h1>Aan de slag als Virtueel Theatermaker</h1>\n<p>Thesis about working in the field of new media and theater. The purpose of the thesis was to map the working field of computers and theatre in order to fetch a blueprint of how to run projects in this area. The thesis is written in Dutch!</p>\n<ul>\n<li><a href=\"_rd_/virtueeltheatermaker/AanDeSlagAlsVirtueelTheatermaker_Loonstra_2005.pdf\">AanDeSlagAlsVirtueelTheatermaker_Loonstra_2005.pdf</a></li>\n</ul>", "_title": "Aan de slag als Virtueel Theatermaker\n", "_date": "20050406\n", "_description": "Thesis about working in the field of new media and theater\n"}, "netsync": {"vnetsync25.zip": "_rd_/netsync", "vnetsync-v1.0b.src.zip": "_rd_/netsync", "body.md": "<h1>Virtools Network Sync Manager</h1>\n<p>This plugin will add master/slave networking to Virtools. It's been created for use in a passive stereoscopic setup and will sync the slave to the master system. It's been created for a research about historical stereophotographics and stereographics for use in theatre performances. This research was initiated by <a href=\"http://www.lange-poten.nl\">Stichting Lange Poten</a>. The plugin was written by <a href=\"http://k-n-p.org\">Knip 'n Plak</a> coder wreck and myself.</p>\n<p><img alt=\"\" src=\"/_rd_/netsync/img/knp.PNG\" /></p>\n<h2>Downloads</h2>\n<ul>\n<li><a href=\"/_rd_/netsync/vnetsync-v1.02b.zip\">VNetSync version 1.02b</a></li>\n<li><a href=\"/_rd_/netsync/vnetsync-v1.01b.zip\">VNetSync version 1.01b</a></li>\n<li>\n<p><a href=\"/_rd_/netsync/vnetsync-v1.0b.zip\">VNetSync version 1.0b</a></p>\n</li>\n<li>\n<p><a href=\"/_rd_/netsync/vnetsync-v1.02b.src.zip\">VNetSync version 1.02b source code</a></p>\n</li>\n<li><a href=\"/_rd_/netsync/vnetsync-v1.01b.src.zip\">VNetSync version 1.01b source code</a></li>\n<li><a href=\"/_rd_/netsync/vnetsync-v1.0b.src.zip\">VNetSync version 1.0b source code</a></li>\n</ul>\n<p>Thanks to Gilles Pinault who provided us with a Virtools Dev 2.5 version.</p>\n<ul>\n<li><a href=\"/_rd_/netsync/vnetsync25.zip\">VNetSync25.zip</a></li>\n</ul>\n<h2>System Requirements</h2>\n<ul>\n<li>Virtools Dev 2.1</li>\n<li>Network interface using TCP protocol</li>\n</ul>\n<h2>Installation notes</h2>\n<p>Just copy the DLL into your 'managers' directory. When you start Virtools you will find a new section 'Building Blocks - Net' in which you'll find two Building Blocks:</p>\n<h3>NS_Master</h3>\n<p><img alt=\"\" src=\"/_rd_/netsync/img/ns_masterbb.gif\" /></p>\n<p>This is the NetSync master Building Block in which you can set the following settings:</p>\n<ul>\n<li>socket_on(boolean): Will enable or disable the socket when playing</li>\n<li>hostname: set the hostname of the slave machine</li>\n<li>port: set the port number to use</li>\n</ul>\n<p>The Building Block accepts a vector and a matrix as input to sent to the slave. It outputs the status of the network manager (master or slave).</p>\n<h3>VNSlaveBB</h3>\n<p><img alt=\"\" src=\"/_rd_/netsync/img/ns_slavebb.gif\" /></p>\n<p>This is the NetSync slave Building Block in which you can set the following settings:</p>\n<ul>\n<li>socket_on(boolean): Will enable or disable the socket when playing</li>\n<li>port: set the portnumber to use</li>\n</ul>\n<p>The Building Block outputs the vector and matrix send by the master. If the master pauses or resets the slave will also stop playing.</p>\n<h2>License &amp; copyrights</h2>\n<p>This plugin is copyright 2004 sphaero.org/KnP/Lange Poten. All rights reserved. The plugin has been released under the Gnu Lesser Public License for others to enjoy.</p>\n<p>We would like to know if people are using this plugin for any purpose. If you make modifications or additions to this plugin you are obliged to provide the source to us to include it into future releases of the plugin.</p>\n<p>Problems or questions please contact us by email</p>\n<h2>Changelog</h2>\n<pre><code># v1.01b 14-01-04\n- fixed mixed up defines\n\n# v1.01b 13-01-04\n- fixed master socket start when not set on\n\n# v1.0b - 09-01-04\n- release date!\n</code></pre>\n<h2>Streaming Camera customisation</h2>\n<p>We've received the following mail from studentrs from the Aalborg University Copenhagen. Their source code is also available <a href=\"/_rd_/netsync/vnettcp_source_send.zip\">here</a>.</p>\n<pre><code>Hi there,\n\nI am a part of a group of Medialogy students from Aalborg University Copenhagen\n(AAUC). We needed the possibility of streaming a processed camera image to a\ntexture in Virtools, and thanks to your open source for VNetSync\n(http://www.sphaero.org/projects/ns/), we managed. Based on that code, we added\ntwo more BBs, called NS_Client and NS_EYWServer, which essentially are examples\nof a TCP client and a TCP Server. Since our solution builds so heavily on your\nproject, we would very much like to see our source code be posted on your web\npage, together with the VNetTCP sources - if possible.\n\nPlease find the package with the source code and example files here (a similar\nversion of this text is also included there as a readme):\nhttp://www.smilen.net/VNetTCP_source_send.zip\n\nNS_Client is meant to connect to a Max/MSP patch, utilising a flashserver object\n(which behaves like a listening TCP server). NS_EYWServer is meant to connect to\nan EyesWeb patch, utilising a SendToNetwork block (which behaves like a TCP\nclient). The current programming reflects those specifics - for instance,\nNS_Client adds trailing \";\"s which flashserver needs, and NS_EYWServer reads 56\nbytes of header that EyesWeb sends at the beginning of each stream. NS_EYWServer\nis currently programmed to receive 128X128 grayscale images from EyesWeb, and\ndisplay them on a texture of the same dimensions in\n\nThe installation is the same as for the VNetSync - the dll has to be placed in\n/Managers folder of Virtools installation. For computers that do not have\nVisual Studio installed, two more dll's are needed in the windows/system32\ndirectory if the BB's are to show up in Virtools - those are msvcp71.dll and\nmsvcr71.dll. The current VNetTCP.dll was compiled with VS .NET 2003 against\nvirtols Dev 3.0. There are example files in the folder, meant to demonstrate\nthe functionality on one computer (via 127.0.0.1 and different ports) although\nwe have run the plugin in a LAN context.\n\nTo test NS_EYWServer, open both VNetTCPTest.cmo in Virtools and\nVNetTCPvinceTestMouse.eyw in EyesWeb (the patch is for EyesWeb 3.3.0). Then\npress play in VNetTCPTest.cmo - Virtools will start blocking (for max 10\nseconds) waiting for a connection. Immediately switch to EyesWeb and start the\npatch - as soon as the connection is established, the received texture from\nEyesWeb (in the example, composed out of the mouse movements) will be rendered\nin Virtools. The connection has to be established during the 10 blocking\nseconds. The ending of the connection between EyesWeb and Virtools is not\nproperly solved so it ends ungraciously - so restart of the EyesWeb patch is\nrequired once a connection has ended. The texture has to be a target parameter\n- it would probably be better to have it as a pOut, if possible. Also, the\nsocket operation was made non-blocking - it would probably be better to have a\nseparate thread run the recv in blocking mode, but we did not go there..\n\nTo test NS_Client, open VNetTCPflashserverTestFader.mxb in Max/MSP first - since\nit will activate the flashserver object, so it starts listening to port 31337.\nThen open and start VNetTCPTest.cmo - since NS_EYWServer is present there, it\nwill still block for 10 seconds upon start, but that is not necesarry to\nconnect to the flashserver in Max. You may want to try deleting the\nNS_EYWServer block to avoid the 10 blocking seconds in this context. Once\nconnected, in VNetTCPTest.cmo you can click the Connect button (which has a\nmisleading name - it simply activates the processing feedback loop of\nNS_Client) to see the received data from max. The data is received as a string,\nalthough here the block also has an integer output - basically, if the string\ndescribes one number, you will get it as an integer as well, and in the example\nin Max, moving the fader creates such a string - and in Virtools, that is mapped\nto the z position of the cube in the example. One can send strings to Max/MSP as\nwell - click the \"Send to Max\" button in the Virtools example.\n\nThe examples can also run together - just run VNetTCPflashserverTestFader.mxb\nfirst, then open VNetTCPTest.cmo, then open VNetTCPvinceTestMouse.eyw - then\nrun the VNetTCPTest.cmo to start the blocking and switch to EyesWeb and start\nthe patch. Once started like this, switch ot Virtools, click on \"Connect\" to\nactivate the feedback loop for processing Max data, and you can see all of them\nin action.\n\nAs we are not all that familiar with the Virtools SDK nor the details of\noperation of VNetSync, a switch was added which simply bypasses Master/Slave\nprocessing if client/EywServer are present - else it should normally continue\nthe Master/Slave operation. Maybe it would be better to separate them somehow,\nso they can run in parallel? - but in any case, the design demands that a\nsingle Network Manager manages them all?\n\nWell, in any case, if you find it worth your while, you are welcome to introduce\nany advances to the code (and we would be happy to see them and learn as well).\nWe would just ask to be listed as \"a group of Medialogy students from Aalborg\nUniversity Copenhagen\" as contributors, if applicable.\n\nAgain, thanks for sharing your great code, and please let me know if we can see\nour sourcecode posted on your page..\n\nBest regards,\nsmilen\n</code></pre>", "vnetsync-v1.0b.zip": "_rd_/netsync", "vnetsync-v1.02b.src.zip": "_rd_/netsync", "_description": "This plugin will add master/slave networking to Virtools.\n", "vnetsync-v1.01b.zip": "_rd_/netsync", "vnetsync-v1.02b.zip": "_rd_/netsync", "_title": "Netsync Manager\n", "img": {"ns_masterbb.gif": "_rd_/netsync/img", "ns_slavebb.gif": "_rd_/netsync/img"}, "vnetsync-v1.01b.src.zip": "_rd_/netsync", "vnettcp_source_send.zip": "_rd_/netsync", "knp.PNG": "_rd_/netsync", "_date": "20040506\n"}, "zocp": {"_abstract": "Trying to control multiple computers in live performances is a challenging task. Often computers intercommunicate using fixed or manual parameters. However when projects expand across many devices this is hard to maintain. Especially in situations where the parameters tend to change. We propose a new protocol which facilitates flexibility and autonomous setups in an orchestrated environment.\n", "_tags": "smart communication, protocol, zocp ", "_author": "Arnaud Loonstra, arnaud@z25.org, Stichting z25.org\n", "_body.textile": "\t<h2>Introduction</h2>\n\n\t<p>As artists embrace new technologies as an artistic medium these technologies often provide artists with new methods for cooperation. Networking technologies are used frequently for these purposes. Internet has been a driving force behind the development of networking hardware and open standards for connecting any to device to any device. These technologies are now a commodity and thus available for anybody to use. However we see a rising need to be able to implement these technologies in a flexible adaptive manner without the explicit fixed setups requiring manual configuration. </p>\n\n\t<p>Open Sound Control (<span class=\"caps\">OSC</span>) is a protocol developed for exchanging music performance data. <span class=\"caps\">OSC</span> is often used as an alternative to <span class=\"caps\">MIDI</span> however it has found its way to many use cases besides musical performances. We have found <span class=\"caps\">OSC</span> to be an ideal de facto standard for connecting applications to each other in order to orchestrate them. However the flexibility of <span class=\"caps\">OSC</span> tends to decrease exponentially when used in large setups because of it&#8217;s hard coded nature. I.e. many applications need to be instructed to use specific manual settings and agreements in order for applications to communicate. In large setups containing multiple systems and multiple applications this manual work is inflexible and error prone.</p>\n\n\t<p>This document describes the results of an initial research into new methods and protocols which are more suited to handle the mentioned situations.</p>\n\n\t<h2>Use cases and requirements</h2>\n\n\t<p>Most networking technologies are designed to cope with almost any situation. Protocols used on the internet are often designed with reliability in mind. They guarantee delivery of the message or they will provide an error. In local networks technologies with different capabilities are found, e.g. transferring the message as fast as possible. As a lot of knowledge has been gained from these existing technologies we are keen on leaning on those resources as much as we can. The first requirements for our foreseen protocol are the following: </p>\n\n\t<ul>\n\t\t<li><span class=\"caps\">KISS</span> (Keep it simple and stupid): We want this protocol not be in our way and we want to be able to understand it easily.</li>\n\t\t<li>Zero Configuration: The protocol should be able to handle most, if not all configuration by itself. There is no need for setting up specific parameters unless requested.</li>\n\t\t<li>Runs on anything TCP/IP: Since TCP/IP is the de facto standard for devices to communicate the technology should be able to run on any device that is able to talk TCP/IP</li>\n\t\t<li>Open Standards: All used technologies, software, protocols should be freely and openly available.</li>\n\t</ul>\n\n\t<p>The use cases for a protocol we foresee are broad. However we initially focus on use cases for live performances and interactive setups. In these setups time is crucial. If a musician is interacting with a device we want the communication to be instant. Any possible delay should be as small as possible. We usually refer to these circumstances as 'realtime&#8217;. When we require the latency to be as small as possible guaranteed message delivery is usually not required. E.g. a corrupted message is discarded since the mechanism to recover from the error takes more time then sending a new message. However there are other use cases when message reliability is important. E.g. An actor is switching a light on. We need to ensure the light switch message is received. Thus both requirements, message latency and message raliability need to be taken into account.</p>\n\n\t<ul>\n\t\t<li>Low latency, when needed</li>\n\t\t<li>Reliability, when needed</li>\n\t</ul>\n\n\t<p>In any development situation one of the most important features is debugging. You want to know what is going on. Especially when you run into situations different from what is expected. In performative or improvising situations this feature is even more important than in regular development situations. When you want technology to be flexible you&#8217;ll need to ensure you can rely on the technology but more important rely on understanding what it is doing and being able to fix it when needed. Meanwhile all this debuging needs to be unintrusive as it cannot intervene with the live situation.</p>\n\n\t<ul>\n\t\t<li>Unintrusive debugging and monitoring</li>\n\t</ul>\n\n\t<h2>Transports</h2>\n\n\t<p>As our main focus is TCP/IP because of its wide availability we have the following available de facto transports:</p>\n\n\t<ul>\n\t\t<li>TCP: For the situations where reliability is required as <span class=\"caps\">TCP</span> guarantees delivery</li>\n\t\t<li>UDP: Where latency is more important</li>\n\t</ul>\n\n\t<p>In order for multiple nodes to exchange data there are 3 network communication topologies to consider:</p>\n\n\t<ul>\n\t\t<li>Unicast for 1:1 communication</li>\n\t\t<li>Multicast for 1:N communication</li>\n\t\t<li>Broadcast for 1:all communication</li>\n\t</ul>\n\n\t<p>These topologies are essential to consider since these topologies are often being handled by hardware networking devices and can deliver very low latency transfers. They can also reduce traffic as a message can be sent once while being received by many. However it is important to understand that most of these communication topologies are not available in Wide Area Networks (<span class=\"caps\">WAN</span>) like the internet. As we can imagine projects to include some exchanging through the internet we need to provide methods to do so while utilizing the efficiency of Local Area Network communication topologies. This essentially means every method topology should be available though a unicast topology since devices communicating across internet can only rely on unicast communication. Only in situations where other topologies are available these can be utilized to provide more efficiency. </p>\n\n\t<p>When in a situation in which a program needs to communicate with an other program running on the same system it will be more efficient to communicate to communicate through other means than a network socket. Examples include an interprocess socket or named pipes. These need to be provided by the host operating system.</p>\n\n\t<ul>\n\t\t<li>Utilize communication topologies provided by hardware devices or host operating system where available</li>\n\t</ul>\n\n\t<p>If all these communication transports need to be accounted for we can conclude that the protocol running on these transports needs to be transport agnostic.</p>\n\n\t<ul>\n\t\t<li>Transport agnostic protocol</li>\n\t</ul>\n\n\t<h2>Decentralisation</h2>\n\n\t<p>One final category to account for is traffic optimizing. In many environments messages are routed through a central device. It acts like the telephone operator connecting you to your requested line. This device becomes a single point of failure as well as a potential bottleneck. In mesh based networks this is essentially not necessary. If one node knows who to talk to it can do so directly without intervention by some central director. This prevents single point of failures as well potential bottlenecks. The protocol therefore needs to contain an 'out of bound&#8217; control protocol which acts in parallel to the data it can transfer. When nodes can communicate directly they also need to know how to connect to each other interfaces. This requires logic in cases where interfaces don&#8217;t match. E.g. output is numbers, input is a string.</p>\n\n\t<ul>\n\t\t<li>Parallel out of bound control protocol</li>\n\t\t<li>Logic for instructing interfaces to connect</li>\n\t</ul>\n\n\t<h2>Existing Technologies:</h2>\n\n\t<p>While researching this new protocol we have studied many existing open technologies in order to prevent reinventing the wheel, find any meeting our requirements or to do cherry picking. Since the list of technologies is too extensive and the technologies itself as well we only mention the technologies we have considered or studied thoroughly as a reference:</p>\n\n\t<ul>\n\t\t<li><a href=\"http://www.mbus.org/\">Mbus</a> The Message Bus (Mbus) is a light-weight local coordination protocol for developing component-based distributed applications that has been developed by Bremen University and University College London. While this was a very interesting candidate it isn&#8217;t suited for internet. </li>\n\t\t<li><a href=\"http://opensoundcontrol.org/introduction-osc\">OSC</a> Open Sound Control is currently the most used protocol in creative applications</li>\n\t\t<li><a href=\"https://www.ietf.org/rfc/rfc2328.txt\">OSPF</a> Open Shortest Path First is routing protocol which does neighbour discovery and exchanges information about networks in order to build routing tables for routers using the most optimal path. This protocol has been studies extensively in order to copy mechanisms it uses. </li>\n\t\t<li><a href=\"http://zeromq.org/\">ZeroMQ</a> A high-performance asynchronous messaging library aimed at use in scalable distributed or concurrent applications</li>\n\t\t<li><a href=\"http://nanoms.org\">NanoMsg</a> Successor to ZeroMQ</li>\n\t\t<li><a href=\"http://dbus.freedesktop.org\">D-Bus</a> D-Bus is a inter-process communication (<span class=\"caps\">IPC</span>) system, allowing multiple, concurrently-running computer programs to communicate with one another.</li>\n\t\t<li><a href=\"http://xmpp.org/\">XMPP</a> <span class=\"caps\">XMPP</span> is the Extensible Messaging and Presence Protocol, a set of open technologies for instant messaging, presence, multi-party chat, voice and video calls, collaboration, lightweight middleware, content syndication, and generalized routing of <span class=\"caps\">XML</span> data.</li>\n\t</ul>\n\n\t<h2>Phase 1: Discovery</h2>\n\n\t<p>The first thing our protocol needs to do is some sort of discovery mechanism. The <span class=\"caps\">OSPF</span> protocol uses a multicast address to discover neighbours and initiate unicast handshake. <span class=\"caps\">DHCP</span> uses broadcast to discover clients and uses unicast to finish a handshake. <span class=\"caps\">MDNS</span> uses multicast to discover services. Bonjour/Avahi is a popular ZeroConf implementation which uses multicast as well. Most mechanism rely on either multicast or broadcast technology. As noted before since these technologies are very uncommon on <span class=\"caps\">WAN</span> networks this limits the use case. In a lab tested prototype we have had very good results using a simple mechanism with multicast. Using this mechanism with a broadcast topology is easy to implement. </p>\n\n\t<p>An other possibility would be to use an existing protocol for doing discovery. The most suitable candidate would be the ZeroConf service like Avahi or Bonjour. We have opted not to do since the scope of these protocols is much wider than our use case. It would make our protocol rely on other software which the operating system would need to provide. In cases where the operating system lacks these services we would need to provide these ourselves. As there is no platform independent solution for these ZeroConf services the dependencies would become overly complicated.</p>\n\n\t<p>While settling on a multicast discovery mechanism our research into existing technologies found a very similar protocol coming from the ZeroMQ community. The Zyre project is a an open-source framework for proximity-based peer-to-peer applications. Zyre has been designed with unreliable wireless networks in mind. Zyre implements Zbeacon which uses a broadcast technology for discovery of nodes. We have have compared our initial discovery prototype with Zyre and concluded that the differences are too small to not embrace this existing technology. However since we would rather rely on multicast technology as opposed to broadcast technology we have joined the ZeroMQ community in order to add this to the Zyre project.</p>\n\n\t<p>Designing our protocol on the foundations of the Zyre project is very beneficial since we can build upon the experience of the ZeroMQ community. This would mean we instantly have a lot of cross platform compatibility and an already tested framework.</p>\n\n\t<p>However we have no discovery mechanism which would work across <span class=\"caps\">WAN</span> networks. This needs to be researched. Our initial research tends to point towards implementing a system based on <span class=\"caps\">DNS</span> using <span class=\"caps\">SRV</span> records like done in the <span class=\"caps\">XMPP</span> protocol. This is a subject of interest for the Zyre project as well. Since in any case manual discovery remains an option we will research this further in the future.</p>\n\n\t<h2>Phase 2: Communication and exchange</h2>\n\n\t<p>After nodes have discovered each other they need to communicate. In any real situation where one enters a room full of people we can imagine one would announce itself and introduce itself personally accompanied by a physical handshake. In the digital realm this is not different. The protocol is able to discover all nodes in its vicinity and so can introduce itself personally to another node and can receive introductions. In a human situation it is a custom to shake hands. In the digital realm we can do a handshake through the network however nodes need information on how to connect to another node. So the discovery beacon contains an address of where the node can be contacted. This address in TCP/IP terms would consist of an ipaddress and a port number.</p>\n\n\t<p>Fortunately the Zyre framework provides exactly this as well. It provides in the broadcast beacon besides some other information the ipaddress and port number of the node&#8217;s unicast socket. </p>\n\n\t<h2>Phase 3: Interfacing</h2>\n\n\t<p>When we would use the protocol as we have described up until here it is usable as a replacement for <span class=\"caps\">OSC</span>. We don&#8217;t need to enter ipaddresses and port numbers anymore. We could just select nodes from a list of discovered nodes and let them communicate like they do now using <span class=\"caps\">OSC</span>. However to fully meet our zero configuration requirement we would like to implement a mechaniscm so nodes tell each other what they are able to send and receive. In order to do so they would need to exchange their capabilities. </p>\n\n\t<p>Again when in a human social situation we would know of means to exchange our capabilities and requirements. One could ask another what they do and so find out how somebody could be useful or how somebody could be of use for another. In the digital realm we don&#8217;t have these etiquettes so we need to agree on a formalisation of one. </p>\n\n\t<p>There are many frameworks to provide Remote Procedure Calls (<span class=\"caps\">RPC</span>) mechanisms. However there are only few that provide mechanisms for exchanging what <span class=\"caps\">RPC</span> calls can be done automatically. Apart from just exchanging <span class=\"caps\">RPC</span> capabilities a formalisation of capabilities would mean nodes can already expect certain capabilities and can act upon them. It would be the basis for autonomous operation of nodes which is also necessary for decentralised operation. However this would be a subject for further research beyond the scope of this initial research. Therefore we settle on a practical initial design of how nodes interface with each other.</p>\n\n\t<p>Before we can start exchanging capabilities we need to agree on what messages look like. We need some serialization format which contains the information of the messages. As there are many options like: <span class=\"caps\">BSON</span>, MessagePack, Protocol Buffers, <span class=\"caps\">OSC</span>, etc. We have settled for <span class=\"caps\">JSON</span> for this research purely for practical reasons. In any further iteration of the design we will focus on the message serialization formats.</p>\n\n\t<p>When we want nodes to communicate there a multitude of possible things they can exchange. We have reduced these options to the following:\n\t<ul>\n\t\t<li>parameters: simple values to get or set on a node</li>\n\t\t<li>signals: event messages emitted from a node</li>\n\t\t<li>sensors: events receivers on a node which can receive signals from a node</li>\n\t\t<li>methods: custom functions which can be called on a node</li>\n\t</ul></p>\n\n\t<p>These possibilities to exchange are all meta data. When this is done using the Zyre framework we are using a reliable unicast transport. However we required to also provide methods for low latency where raliability is not of concern. Therefore we provide a mechanism to completely bypass the protocol so direct communication between programs is made possible. This would be the best setup for any low latency situation as it would be done using custom sockets. This mechanism would also provide a method to wrap other protocols inside our protocol. E.g. one could wrap a video data stream through an <span class=\"caps\">RTP</span> socket. For this mechanism we provide the following extra options:\n\t<ul>\n\t\t<li>sources: data emitted from the node through a custom socket using an alien protocol</li>\n\t\t<li>sinks: receiver for data emitted from a custom socket using an alien protocol</li>\n\t</ul></p>\n\n\t<p>The requirements for exchanging capabilities would then become:\n\t<ul>\n\t\t<li>provide information for all capabilty objects of the node</li>\n\t\t<li>provide methods to get and modify parameters of these objects</li>\n\t\t<li>provide methods to call methods on these objects</li>\n\t\t<li>provide methods to subscribe or unsubscribe signals to/from sensors of these objects</li>\n\t\t<li>provide methods to subscribe or unsubscribe streams to/from sinks of these objects</li>\n\t</ul></p>\n\n\t<p>A node&#8217;s capability would be a tree-like data structure containing all relevant information. E.g:<br /><pre>\nnode:          name\n  root:        physical base properties where node is located\n  objects      objects accessible through this node\n    object     object properties and data\n      signals  signals emitted from this object\n      sensors  available sensors on this object\n      methods  available methods to be called on this object\n      sinks    available stream sinks at this object\n      sources  available stream sources at this object\n      ...      any data belonging to the object e.g.\n      localMat local matrix\n      type     type of object (camera, processor, projector, etc)\n      visible  visibility state\n</pre></p>\n\n\t<p>Of course we have researched exiting technologies. We have found many <span class=\"caps\">RPC</span> and <span class=\"caps\">IPC</span> technologies and found D-Bus closest to meeting our requirements. We are currently investigating if we want to use D-Bus fully or if we want to map D-Bus onto our design.</p>\n\n\t<h2>Initial implementation</h2>\n\n\t<p>During this research we have implemented a prototype. The results have been published on Github:</p>\n\n\t<p><span class=\"caps\">ZOCP</span> implementation in Python:\n\t<ul>\n\t\t<li>http://github.com/z25/pyZOCP</li>\n\t</ul></p>\n\n\t<p>Pyre an implementation of Zyre in Python:\n\t<ul>\n\t\t<li>http://github.com/zeromq/pyre</li>\n\t</ul></p>\n\n\t<h2>Conclusions</h2>\n\n\t<p>Our research has resulted in an initial design for a protocol providing a way for systems to automatically discover each other, exchange capabilities and provide mechanism for setting up communication. The design provides methods for incorporating any other protocols like the legacy <span class=\"caps\">OSC</span> protocol. It therefore provides a migration path towards fully implementing this protocol in current software. As this design has been developed inside a laboratory environment it still needs to be tested thoroughly in practical situations. New insights gained from these tests will result in modifications of the design. However we believe the design is a sane foundation to meet our set requirements.</p>\n\n\t<h3>Future work</h3>\n\n\t<p>Multiple areas for future research are mentioned:\n\t<ul>\n\t\t<li>utilizing multicast and broadcast topologies </li>\n\t\t<li>discovery mechanisms on <span class=\"caps\">WAN</span> networks</li>\n\t\t<li>autonomous interfacing and exchange</li>\n\t\t<li>serialization formats</li>\n\t\t<li>implementing or mapping D-Bus</li>\n\t\t<li>researching logic for connecting interfaces</li>\n\t</ul></p>\n\n\t<p>The current prototype has embraced the Zyre (<span class=\"caps\">ZRE</span>) framework as the basis. There are still some design considerations which need to be looked into. One important aspect is our wish to design the protocol as close to hardware implementations as possible. One practical area of research for <span class=\"caps\">ZRE</span> would be to compare Zyre&#8217;s group messaging system to network multicast technologies like <span class=\"caps\">IGMP</span>. </p>\n\n\t<p>Lastly, discovery in it&#8217;s current state will only operate on networks enabled with multicast routing or it will be limited to the local segment of the network. This is a subject for the Zyre project as well which we hope to tackle together with the ZeroMQ community. </p>\n\n\t<p>Any future iteration of the protocol will be published in repositories owned by the Z25 Foundation. Currently these can be found on Github.</p>\n\n\t<h2>References</h2>\n\n\t<ul>\n\t\t<li><a href=\"http://projects.z25.org/projects/plab/wiki/OrchestratorControlProtocolSpec\"><span class=\"caps\">ZOCP</span> specification</a> http://projects.z25.org/projects/plab/wiki/OrchestratorControlProtocolSpec Retrieved on 2014-03-06</li>\n\t\t<li><a href=\"http://www.itcertnotes.com/2011/02/ospf-neighbor-establishment-process.html\"><span class=\"caps\">OSPF</span> discovery</a> http://www.itcertnotes.com/2011/02/ospf-neighbor-establishment-process.html Retrieved on 2014-03-06</li>\n\t\t<li><a href=\"http://lists.zeromq.org/pipermail/zeromq-dev/attachments/20130502/75df19e8/attachment.pdf\"><span class=\"caps\">CERN</span> ZeroMQ review</a> http://lists.zeromq.org/pipermail/zeromq-dev/attachments/20130502/75df19e8/attachment.pdf Retrieved on 2014-03-06</li>\n\t\t<li><a href=\"http://hintjens.com/blog:32\">Solving the discovery problem</a> http://hintjens.com/blog:32 Retrieved on 2014-03-06</li>\n\t\t<li><a href=\"http://zguide.zeromq.org/php:chapter8\"><span class=\"caps\">ZYRE</span> design</a> http://zguide.zeromq.org/php:chapter8 Retrieved on 2014-03-06</li>\n\t</ul>", "_status": "completed\n", "_title": "Orchestrating computer systems, a research into a new protocol", "_date": "20140306\n"}, "gameanalyse": {"body.md": "<h1>Game Analysis: Ideologica?</h1>\n<p>A game concept which was created after studying analysis methods which could be used to analyse video games. The idea was not to analyse a game using analysis methods but to create a game while going through several analysis methods. </p>\n<p>The document is available in the Dutch language:</p>\n<ul>\n<li><a href=\"_rd_/gameanalyse/gameanalyseconcept.pdf\">gameanalyseconcept.pdf</a></li>\n</ul>", "_description": "A game concept which was created after studying analysis methods which could be used to analyse video games. The idea was not to analyse a game using analysis methods but to create a game while going through several analysis methods.\n", "gameanalyseconcept.pdf": "_rd_/gameanalyse", "gameanalyseconcept.sxw": "_rd_/gameanalyse", "_title": "Ideologica\n", "_date": "20030203\n"}, "pptpmxstream": {"body.md": "<h1>PPtP-MxStream</h1>\n<p>I started this little project since I couldn't find an easy package to connect a Linux system with MxStream ADSL back in 2000. Since I liked the RPM package system I created an RPM which makes it easy for anyone to make a pptp connection and to maintain the connection.</p>\n<p>The project is completely obsolete nowadays. As it was only relevant in The Netherlands the project was in Dutch.</p>\n<p>The downloads are not available anymore!</p>\n<h2>Downloads</h2>\n<p>Zie voor installatie instructies <a href=\"/_rd_/pptpmxstream/installatie.html\">hier</a></p>\n<h3>pptp-mxstream-1.0.3-2.2</h3>\n<p>Deze versie is hetzelfde als de versie van R.Klazes echter heb ik nog een voorbeeld en een config bestand er aan toegevoegd. Ik heb niets in de source veranderd. Deze versie is echter gericht op de MxStream verbinding. Er wordt na de installatie kort uitgelegd wat er nog gebeuren moet om de MxStream verbinding tot stand te krijgen. Vanaf release 2.0 is er ook een SysV script toegevoegd waardoor de verbinding tijdens het booten volgens de SysV methode gestart kan worden. (Middels LinuxConf is dit bijvoorbeeld in te stellen).</p>\n<ul>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.i386.rpm\">pptp-mxstream-1.0.3-2.2.i386.rpm</a></li>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.i486.rpm\">pptp-mxstream-1.0.3-2.2.i486.rpm</a></li>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.i586.rpm\">pptp-mxstream-1.0.3-2.2.i586.rpm</a></li>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.i686.rpm\">pptp-mxstream-1.0.3-2.2.i686.rpm</a></li>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.k6.rpm\">pptp-mxstream-1.0.3-2.2.k6.rpm</a></li>\n</ul>\n<p>Een tarball van de i386 rpm.</p>\n<ul>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.tgz\">pptp-mxstream-1.0.3-2.2.tgz</a></li>\n</ul>\n<p>Als u niet weet welke versie u moet hebben, kiest u gewoon de 'i386' versie dan is het altijd goed.</p>\n<p>Sources:</p>\n<ul>\n<li><a href=\"pptp-mxstream-1.0.3-2.2.src.rpm\">pptp-mxstream-1.0.3-2.2.src.rpm</a> (een tarball is aanwezig in deze rpm)</li>\n</ul>\n<h3>pptp-linux-1.0.3-rk2 (originele versie van R.Klazes)</h3>\n<p>Deze versie is direct gecompileerd van de versie van R.Klazes.</p>\n<ul>\n<li><a href=\"pptp-linux-1.0.3-1.i386.rpm\">pptp-linux-1.0.3-1.i386.rpm</a></li>\n<li><a href=\"pptp-linux-1.0.3-1.i486.rpm\">pptp-linux-1.0.3-1.i486.rpm</a></li>\n<li><a href=\"pptp-linux-1.0.3-1.i586.rpm\">pptp-linux-1.0.3-1.i586.rpm</a></li>\n<li><a href=\"pptp-linux-1.0.3-1.i686.rpm\">pptp-linux-1.0.3-1.i686.rpm</a></li>\n</ul>\n<p>Sources:</p>\n<ul>\n<li><a href=\"pptp-linux-1.0.3-1.src.rpm\">pptp-linux-1.0.3-1.src.rpm</a></li>\n<li><a href=\"pptp-linux-1.0.3-rk2.tgz\">pptp-linux-1.0.3-rk2.tgz</a></li>\n</ul>\n<h3>pptp-mxstream (oudere versies)</h3>\n<p>(not available anymore)</p>\n<ul>\n<li>pptp-mxstream-1.0.3-2.1.i386.rpm</li>\n<li>pptp-mxstream-1.0.3-2.1.i486.rpm</li>\n<li>pptp-mxstream-1.0.3-2.0.i386.rpm</li>\n<li>pptp-mxstream-1.0.3-2.0.i486.rpm</li>\n<li>pptp-mxstream-1.0.3-1.1.i386.rpm</li>\n<li>pptp-mxstream-1.0.3-1.1.i486.rpm</li>\n<li>pptp-mxstream-1.0.3-1.1.i586.rpm (N/A)</li>\n<li>pptp-mxstream-1.0.3-1.1.i686.rpm (N/A)</li>\n</ul>\n<p>Sources:</p>\n<ul>\n<li>pptp-mxstream-1.0.3-2.1.src.rpm</li>\n<li>pptp-mxstream-1.0.3-2.0.src.rpm</li>\n<li>pptp-linux-1.0.3-1.1.src.rpm (een tarball is aanwezig in deze rpm's)</li>\n</ul>\n<h3>07-12-'01 release 2.2 (final)</h3>\n<p>Waarschijnlijk de final release. Ik ben niet van plan nog dingen te veranderen. Ik heb wel toenaderingen gehad om extra features aan het pakket toe te voegen. Ik heb die niet toegevoegd aangezien het pakket is waarvoor het dient. Gewoon doen wat het moet doen en geen extra toeters en bellen. Ik overweeg nog om een firewall pakket te schrijven maar de komende tijd focus ik me op een nieuw project.</p>\n<ul>\n<li>Upgrade schrijft niet meer /etc/ppp/peers/mxstream over.</li>\n<li>SysV script controleert nu voor het stoppen of de ppp link nog wel bestaat</li>\n<li>CPU geoptimaliseerde versies</li>\n</ul>\n<h3>30-07-'01 release 2.1</h3>\n<ul>\n<li>SysV script verbeterd. De verbinding wordt nu netjes verbroken bij het stop commando.</li>\n<li>Een eigen logfile toegevoegd. Alles wordt nu naast standaard syslog ook gelogd naar /var/log/mxstream</li>\n</ul>\n<h3>17-07-'01 release 2.0</h3>\n<ul>\n<li>Een eerste SysV script toegevoegd. Middels dit script kan de verbinding gestart worden tijdens het opstarten. Het script wordt herkent door chkconfig en dus ook Linuxconf.</li>\n</ul>\n<p>Gelijk ook maar de installatie instructies ge-update</p>\n<h3>09-07-'01 release 1.1</h3>\n<ul>\n<li>Een aantal fouten in de SPEC file verwijderd</li>\n<li>Opnieuw gecompileerd omdat ik geen make clean had gedaan</li>\n</ul>\n<h3>08-07-'01 Rpm's gebakken release 1</h3>\n<p>Na een goed bevallen test met de pptp versie van R.Klazes besloten om rpm's van zijn versie te maken. Inmiddels twee verschillende versies:</p>\n<ul>\n<li>ongemodifeerde versie van pptp-1.0.3-rk2</li>\n<li>een versie waarbij ik alvast enkele config files toegevoegd evenals een voorbeeld. Alles bedoeld voor MxStream (pptp-mxstream-1.0.3)</li>\n</ul>\n<p>Zie download sectie voor de betreffende archieven.</p>\n<h2>Credits</h2>\n<p>Natuurlijk alle credits naar R.Klazes en het originele pptp team. En de mensen die mij wezen op fouten die ik maakte of met tips voor extra functionaliteit.</p>", "installatie.html": "_rd_/pptpmxstream", "img": {"mxstream.png": {"web": "mxstream_web.jpg", "thumb": "mxstream_thumb.jpg", "md5": "todo"}}, "_description": "An easy package to connect a Linux system with MxStream ADSL back in 2000.\n", "_title": "PPtP MxStream\n", "_date": "20000101\n"}}, "index.html": "", "projects": {"emptyeyes": {"img": {"ee-02.jpg": "projects/emptyeyes/img", "ee-04.jpg": "projects/emptyeyes/img", "final2.jpg": "projects/emptyeyes/img", "ee-05.jpg": "projects/emptyeyes/img", "ee-08.jpg": "projects/emptyeyes/img", "ee-07.jpg": "projects/emptyeyes/img", "ee-06.jpg": "projects/emptyeyes/img", "final1.jpg": "projects/emptyeyes/img", "final3.jpg": "projects/emptyeyes/img", "final5.jpg": "projects/emptyeyes/img", "ee-03.jpg": "projects/emptyeyes/img", "final4.jpg": "projects/emptyeyes/img", "final6.jpg": "projects/emptyeyes/img", "ee-09.jpg": "projects/emptyeyes/img", "ee-01.jpg": "projects/emptyeyes/img"}, "_description": "Digital visualisation of scupltures by Jan Samsom\n", "_body.textile": "These sculptures \"Empty Eyes\":https://nl.wikipedia.org/wiki/Empty_eyes are designed by \"Jan Samsom\":http://www.jan-samsom.nl/index.php?page=200bussum. It was an art assignment for the roundabout near 'Clinge' Bussum, The Netherlands. Because of the complexity of the objects a conventional maquette wasn't suitable. With Jan I created a virtual maquette of the surrounding and made virtual photo's of it.\n\nAt the request of the jury of this project we've made pictures of how the objects would look in the environment when finished. \n", "_status": "completed\n", "_title": "Empty Eyes\n", "_date": "20020606\n"}, "cosmogirl": {"vprodorst.video": "projects/cosmogirl", "img": {"cosmo2.png": {"web": "cosmo2_web.jpg", "thumb": "cosmo2_thumb.jpg", "md5": "todo"}, "cosmo3.png": {"web": "cosmo3_web.jpg", "thumb": "cosmo3_thumb.jpg", "md5": "todo"}, "cosmo0.png": {"web": "cosmo0_web.jpg", "thumb": "cosmo0_thumb.jpg", "md5": "todo"}, "cosmo1.png": {"web": "cosmo1_web.jpg", "thumb": "cosmo1_thumb.jpg", "md5": "todo"}}, "_description": "Experimental realtime immersive visuals complementing Cosmo V performance\n", "teaser.video": "projects/cosmogirl", "teaser.vimeo": "projects/cosmogirl", "_body.textile": "h1. Cosmo V Experience\n\n\"Cosmo V\":http://cosmov.bandcamp.com/ is a friend of mine with quite a succesful release album of 'bedroompop' songs. Her music is inspired by artists like Christa P\u00e4ffgen (Nico), Charles Bukowski, Daul Kim and Jim Morisson. For her perfomances we created an experimental immersive environment using curved projections. Artists from \"z25\":http://www.z25.org experimented with visuals using the songs from Cosmo V. \n\nThe visuals were controlled just like a musical instrument and by doing so became a part of the band. Realtime applications like this use a lot of techniques which are used for video games. This is why we chose to put the \"Blender Game Engine\":http://www.blender.org to the test.\n\nPerformances were played at:\n* Muziekgebouw, Utrecht\n* DBase, Utrecht\n* Overtoom 301, Amsterdam\n* Tivoli, Utrecht\n* Muziekgebouw aan 't Ij/Bimhuis, Amsterdam\n", "_status": "completed", "_title": "Cosmo V Experience", "_date": "20120427"}, "lili": {"img": {"IMG_9875.png": {"web": "IMG_9875_web.jpg", "thumb": "IMG_9875_thumb.jpg", "md5": "todo"}, "IMG_3506.png": {"web": "IMG_3506_web.jpg", "thumb": "IMG_3506_thumb.jpg", "md5": "todo"}, "IMG_9897.png": {"web": "IMG_9897_web.jpg", "thumb": "IMG_9897_thumb.jpg", "md5": "todo"}, "IMG_9851.png": {"web": "IMG_9851_web.jpg", "thumb": "IMG_9851_thumb.jpg", "md5": "todo"}, "IMG_3425.png": {"web": "IMG_3425_web.jpg", "thumb": "IMG_3425_thumb.jpg", "md5": "todo"}, "IMG_3478.png": {"web": "IMG_3478_web.jpg", "thumb": "IMG_3478_thumb.jpg", "md5": "todo"}, "IMG_3510.png": {"web": "IMG_3510_web.jpg", "thumb": "IMG_3510_thumb.jpg", "md5": "todo"}}, "_description": "Opening the windows of an historic barrack to a world of it's former occupants.\n\n\n\n", "_body.textile": "h1. Linie in Lichterlaaie\n\n\"Fort Blauwkapel\" is part of the New Dutch Water Line in Utrecht. For the \"Linie in Lichterlaaie\" festival wel created a very simple distributed setup to project on all the windows of the building. We created some program logic to control Raspberry Pi devices attached to video projectors from within \"Blender\":http://www.blender.org. This was no rocket science but the simplicity of the system proofed to be very powerfull. Especially for artists wanting to play with massive projections.\n\nWe created a 3D photo collage of soldiers who used this building around the first World War.\n\nThe military barracks became a visual time machine which brought back a part of Dutch war history.\n\nThis project was realised in cooperation with the \"MAPLAB\":http://maplab.nl \n", "_status": "completed", "lili-long.video": "projects/lili", "_title": "Linie in Lichterlaaie", "_date": "20130420\n"}, "bewogen": {"img": {"2.jpg": "projects/bewogen/img", "3.jpg": "projects/bewogen/img", "4.jpg": "projects/bewogen/img", "1.jpg": "projects/bewogen/img"}, "_description": "A king is searching for emotion and builds a theatre, but he uses some dodgy material.\n", "_body.textile": "h1. Bewogen\n\nAn animation based on the theme 'Duet'. The story is about a 'King' who has no emotion. He is searching for it. For this he has built himself a theatre in which he has built a performance act. He's using some dodgy material to built it's theatre, though!\n\nThe story is based on some ideas regarding the world we live in and the relation we have to her.\n\nBewogen was selected for the \"Holland Animation Film Festival 2004\":http://www.haff.nl/en/films/bewogen\n", "_status": "completed\n", "_title": "Bewogen\n", "_date": "20030101\n"}, "bartje": {"img": {"3-400x300.jpg": "projects/bartje/img", "6-400x300.jpg": "projects/bartje/img", "8-400x300.jpg": "projects/bartje/img", "9-400x300.jpg": "projects/bartje/img", "5-400x300.jpg": "projects/bartje/img", "7-400x300.jpg": "projects/bartje/img", "1-400x300.jpg": "projects/bartje/img", "2-400x300.jpg": "projects/bartje/img", "4-400x300.jpg": "projects/bartje/img"}, "_description": "A knitting super hero\n", "_body.textile": "h1. Bartje\n\nActions and action stories mostly point at a main character or a group of main characters. The big problem they always bump into can only be solved by way of smaller problems that find their definition of the problem and its solution in a certain action. These actions can be pointed at the environment or other characters, or a combination of these.\n\nFor this project we created a visual story with a superhero as a main character. Our Superhero is a guy living in a village with his grandfather. He loves to knit but is always bullied for that reason. When his grandfather dies he decides to leave the village. During his journey he meets all kinds of small problems which he solves with his knitting power. He ends up living with a beer family. During winter the bears are having their wintersleep so he decides to go back to the village where he came from. Back in the village a major disaster is happening so our superhero is there to save the day.\n\nAll our scenes are made from clay and digitally enhanced. \n", "_status": "completed\n", "_title": "Bartje\n", "_date": "19990909\n"}, "torenvanbabbel": {"img": {"tordinaire02.png": {"web": "tordinaire02_web.jpg", "thumb": "tordinaire02_thumb.jpg", "md5": "todo"}, "tordinaire03.png": {"web": "tordinaire03_web.jpg", "thumb": "tordinaire03_thumb.jpg", "md5": "todo"}, "bilderraus01.png": {"web": "bilderraus01_web.jpg", "thumb": "bilderraus01_thumb.jpg", "md5": "todo"}, "tordinaire01.png": {"web": "tordinaire01_web.jpg", "thumb": "tordinaire01_thumb.jpg", "md5": "todo"}, "tordinaire04.png": {"web": "tordinaire04_web.jpg", "thumb": "tordinaire04_thumb.jpg", "md5": "todo"}}, "_description": "With a group of artists and the latest techniques the z25.org foundation pulls out all the stops with 'Tordinaire'.\n", "tordinaire.video": "projects/torenvanbabbel", "_body.textile": "h1. Tordinaire\n\nAfter 2011's edition of \"VJ op de Dom\":http://vjopdedom.nl z25 was asked to once more create a production for this event hosted in the centre of Utrecht, The Netherlands. As a sequel to Bilderraus, in this production the surroundings were also embraced by controlling all lighting and thus creating a complete emersive show.\n\nAll artists were asked to use the mythological story of the Tower of Babel combined with modern international politics as an inspirational theme. With the diverse content created by the artists a recurring constructing and imploding storyline is shown.\n\nFor this project I participated as one of the artists (@9:57 in the video) and also did most of the technological research together with Aldo Hoeben.\n\nh2. Tordinaire Visual Artists\n\n* \"Aldo Hoeben\":http://www.fieldofview.com\n* \"Ingrid Govers\":http://www.ingridgovers.nl\n* \"Yvonne Dubbers\":http://theatergroepkrabben.nl\n* \"David Middendorp\":http://www.davidmiddendorp.nl\n* \"Roderick Gadellaa\":http://www.rejh.nl\n* \"Machiel Veltkamp\":http://www.cpu-theatre.org\n* \"Arnaud Loonstra\":http://www.sphaero.org\n* \"Studio Zesbaans\":http://www.zesbaans.nl\n* \"Veerle Cima\":http://vimeo.com/veerle/videos\n\nh2. Tordinaire Audio Artists:\n\n* \"Tijs Ham\":http://www.tapage-sound.com \n* \"Mark Ijzerman\":http://markijzerman.com/\n\nh2. Directors:\n\n* \"Jan Samsom\":http://www.jan-samsom.nl\n* \"Marcel Alberts\":http://www.marcelalberts.nl\n\n*Performed on September 21st 2012*\n\nh2. Photography:\n\n* \"Rogier Veldman\":http://www.rogierveldman.nl/\n* \"Anna van Kooij\":http://www.annafotografie.nl/\n\nh2. Sponsorship:\n\n!http://www.fentenervanvlissingenfonds.nl/style/fentener/images/logo-50.png(Fentener van Vlissingenfonds)!\n", "_status": "completed", "_title": "Tordinaire\n", "_date": "20120921"}, "twv": {"img": {"twv-01.jpg": "projects/twv/img", "twv-03.jpg": "projects/twv/img", "twv-02.jpg": "projects/twv/img"}, "_description": "A videoclip and song based on 'smartlappen'\n", "twv.video": "projects/twv", "_body.textile": "h1. 't Water Voorbij\n\nThis videoclip was created during Art Academy. I was asked to do a full autonomous project. Therefore, I started studying the history of the 'smartlap', a typical local music genre. My goal was to write my own 'smartlap'. Although the final product isn't a typical 'smartlap', it does follow the conventions of the smartlap. Through my study of the history of the 'smartlap' I found that the 'smartlap' has a broader range than we would think at first. Because time was on my side I also decided to create a video for the soundtrack.\n\nThe song is about a young man who sings about his despair after his girlfriend decided to end her struggle. The song is entirely in Dutch.\n\nThe music was written by Maarten van der Bijl and myself. Very special thanks to Matthew Groen for his assistance in directing the project.\n", "_status": "completed\n", "_title": "'t Water Voorbij\n", "_date": "20030601\n"}, "il": {"img": {"il5.jpg": "projects/il/img", "il2.jpg": "projects/il/img", "il1.jpg": "projects/il/img", "il4.jpg": "projects/il/img", "il3.jpg": "projects/il/img"}, "_description": "A philosphically themed animation with a shimmer of the mythological story of Icarus\n", "il.video": "projects/il", "_body.textile": "You are introduced to an underwater scene. A lightbeam awakes 2 creatures lying on the bottom of the seafloor. Out of curiosity they swim along the lightbeam to the surface. The most enthousiastic creature jumps out of the water and continous to fly towards the light. The other stays behind but joins the other when he comes back to get him. Together they fly towards the light until one of them gets a bit too close. \n\nIn this story shimmer a few philosphical theme's as well as the mythological story of Icarus. \n\nThe animation was insprired by the art of Dance. Dance and movement speak their own language, an expressive language that is used in theater as much as spoken language or text. At the same time this expressive language forms the enviroment in a physical way, more than the spoken word. The enviroment influence the choreography and visa versa.\n\nIntentio Luminis was selected for the \"Holland Animation Film Festival\":http://www.haff.nl/en/films/intentio-luminis which was held from 13 till 17 November 2002 in Utrecht, The Netherlands.\n", "_status": "completed\n", "_title": "Intentio Luminis\n", "_date": "20021113\n"}, "farpoint": {"img": {"P1070677.png": {"web": "P1070677_web.jpg", "thumb": "P1070677_thumb.jpg", "md5": "todo"}, "P1070672.png": {"web": "P1070672_web.jpg", "thumb": "P1070672_thumb.jpg", "md5": "todo"}, "P1070659.png": {"web": "P1070659_web.jpg", "thumb": "P1070659_thumb.jpg", "md5": "todo"}, "P1070569.png": {"web": "P1070569_web.jpg", "thumb": "P1070569_thumb.jpg", "md5": "todo"}, "P1070589.png": {"web": "P1070589_web.jpg", "thumb": "P1070589_thumb.jpg", "md5": "todo"}, "P1070656.png": {"web": "P1070656_web.jpg", "thumb": "P1070656_thumb.jpg", "md5": "todo"}, "P1070572.png": {"web": "P1070572_web.jpg", "thumb": "P1070572_thumb.jpg", "md5": "todo"}, "P1070601.png": {"web": "P1070601_web.jpg", "thumb": "P1070601_thumb.jpg", "md5": "todo"}, "P1070524.png": {"web": "P1070524_web.jpg", "thumb": "P1070524_thumb.jpg", "md5": "todo"}, "P1070513.png": {"web": "P1070513_web.jpg", "thumb": "P1070513_thumb.jpg", "md5": "todo"}, "P1070517.png": {"web": "P1070517_web.jpg", "thumb": "P1070517_thumb.jpg", "md5": "todo"}, "P1070559.png": {"web": "P1070559_web.jpg", "thumb": "P1070559_thumb.jpg", "md5": "todo"}}, "_description": "An explorative data visualization pilot for the Deloitte Utrecht iZone\n", "vid2-httpserver.video": "projects/farpoint", "vid1-theglobe.video": "projects/farpoint", "_body.textile": "h1. The Globe\n\nI worked on this project in conjunction with \"Stichting z25.org\":http://www.z25.org, the \"Media And Performance Lab\":http://www.maplab.nl (MAPLAB) to create a data visualisation pilot for the iZone, an environment at the Utrecht office of \"Deloitte\":http://www.deloitte.nl which stimulates to think and act differently through it's innovative design.\n\nWe created a concept for experiencing data interactively and immersively. For this pilot we used open data from \"Eurostat\":http://epp.eurostat.ec.europa.eu/portal/page/portal/eurostat/home/ consisting of EU Industrial R&D Investment ScoreBoard from 2002 until 2010.\n\nThe visualisation consists of a realtime 3D world which you can browse and control with a tablet to explore the different data on R&D investments filtered by year, industry branch, continent or country. By making this available interactively, users can validate and compare different scenarios in new explorative ways.\n\nh2. Technology\n\nWe chose the \"Blender Game Engine\":http://blender.org (BGE) as the realtime 3D engine to run the actual visualisation, displayed on the iZone's so called _Experience Wall_, an 8 meter wide, high resolution screen. It consists of 12 full HD screens and has a total resolution of 5760 x 1080.\n\nThe tablet app is a webapp which implements jQuery and ajax to communicate with an http server running in the BGE.\n\nh3. z25 Blender HTTP Server\n\nTo establish the connection between the web app and Blender we developed an HTTP server with a REST API that runs in BGE. This server is automatically aware of all the objects in a given scene by means of introspection. This means that with this webservice it is possible to retrieve information on all objects in a scene and manipulate their characteristics remotely.\n\nA standalone version of the Blender HTTP Server can be found on github and includes a working HTTP server with example Blender scene and web client and documentation.\n\n* \"BGE HTTP Server git repository\":https://github.com/z25/z25-blender-http-server\n\nh2. Credits\n\n* Deloitte Innovation Utrecht\n* HKU MAPLAB\n* Stichting z25.org\n", "_status": "completed", "_title": "The Globe", "_date": "20121217\n"}, "bodycount": {"img": {"bodycount_7.png": {"web": "bodycount_7_web.jpg", "thumb": "bodycount_7_thumb.jpg", "md5": "todo"}, "bodycount_12.png": {"web": "bodycount_12_web.jpg", "thumb": "bodycount_12_thumb.jpg", "md5": "todo"}, "bodycount_10.png": {"web": "bodycount_10_web.jpg", "thumb": "bodycount_10_thumb.jpg", "md5": "todo"}, "bodycount_8.png": {"web": "bodycount_8_web.jpg", "thumb": "bodycount_8_thumb.jpg", "md5": "todo"}, "bodycount_9.png": {"web": "bodycount_9_web.jpg", "thumb": "bodycount_9_thumb.jpg", "md5": "todo"}, "bodycount_11.png": {"web": "bodycount_11_web.jpg", "thumb": "bodycount_11_thumb.jpg", "md5": "todo"}, "bodycount_4.png": {"web": "bodycount_4_web.jpg", "thumb": "bodycount_4_thumb.jpg", "md5": "todo"}, "bodycount_5.png": {"web": "bodycount_5_web.jpg", "thumb": "bodycount_5_thumb.jpg", "md5": "todo"}, "bodycount_6.png": {"web": "bodycount_6_web.jpg", "thumb": "bodycount_6_thumb.jpg", "md5": "todo"}, "bodycount_13.png": {"web": "bodycount_13_web.jpg", "thumb": "bodycount_13_thumb.jpg", "md5": "todo"}, "bodycount_1.png": {"web": "bodycount_1_web.jpg", "thumb": "bodycount_1_thumb.jpg", "md5": "todo"}, "bodycount_2.png": {"web": "bodycount_2_web.jpg", "thumb": "bodycount_2_thumb.jpg", "md5": "todo"}, "bodycount_3.png": {"web": "bodycount_3_web.jpg", "thumb": "bodycount_3_thumb.jpg", "md5": "todo"}}, "_description": "How do we relate to the virtual world? In what way can we identify ourselves with and feel responsible for our digital selves? \n", "responsible_hd.video": "projects/bodycount", "_body.textile": "h2. Respons(e)ible Project\n\nHow do we relate to the virtual world? In what way can we identify ourselves with and feel responsible for our digital selves? What happens when we cannot control our digital selves? What has been taken away from you? The Resons(e)ible Project is a theatrical research to responsibility in the virtual world. A cooperation of the \"HKU\":http://www.talltreelabs.org/ and z25.org Foundation. We created a theatrical experience in which we copied the audience to the virtual world by using 3d visualization and scanning techniques.\n\nThe project was performed at \u201cSoiree des Ateliers\u201d in Huis aan de Werf, Utrecht on 14, 15 & 16 february 2008 and in Frascati, Amsterdam on 11,16,17,18 September 2010.\n\nh2. Credits \n\nI realized The Respons(e)ible Project with my estimeed colleagues:\n\n* \"Bas Haas\":http://www.virtuater.com\n* \"Joris Weijdom\":http://www.talltreelabs.org\n\nand wouldn't have been possible without the volunteers:\n\n* Marja Kok\n* Jelte van der Maat\n* Matthew groen\n* Jelle van der Ster\n* Cindy van Rooijen\n* Elmer Zwolsman\n* Saskia Collard\n* Lavie Schimanski\n* Roy Munsterman\n* Annemieke Ros\n* Esmeralda Detmers\n* Rianne Meboer\n* Marjolijn Zwakman\n* Pieter Albers\n* Merijn Vogelsang\n* Roderik Biersteker\n* Ferdy Guliker\n\nh3. Thanks to:\n\n* \"Huis aan de Werf\":http://www.huisaandewerf.nl\n* \"Rob Kerp - DJ Group\":http://www.djgroup.nl\n* Felix Heijnen - Kunstenarij\n* \"Gerard Wunsch - Redream\":http://www.redream.nl\n* \"Xform\":http://www.xform.nl\n", "_status": "completed", "_title": "Respons(e)ible Project", "_date": "20080217\n"}, "takt": {"img": {"2.jpg": "projects/takt/img", "8.jpg": "projects/takt/img", "6.jpg": "projects/takt/img", "4.jpg": "projects/takt/img", "7.jpg": "projects/takt/img", "5.jpg": "projects/takt/img", "3.jpg": "projects/takt/img", "1.jpg": "projects/takt/img"}, "_description": "Life musical visualization using a game engine\n", "_body.textile": "h1. TAKT life musical visualization\n\nIn this project we were asked to do visualization of live music composed by Thys Dercksen. We created a virtual environment which reacted to the music and was autonomous in it's control. We tried not make an abstract visualization but tried to tell a story of a girl in hospital. For this we had to write a program which could listen to and analyze live music. We also invited 2 persons from the public who would influence the performance with controls.\n\nThis was performed on the 9th of March 2003 in Theater De Kom in Nieuwegein, The Netherlands. The music was composed by Thys Dercksen and performed by Thys Derksen and Annie Tangberg. \n", "_status": "completed\n", "_title": "TAKT\n", "_date": "20030309\n"}, "out": {"img": {"3-400x300.jpg": "projects/out/img", "6-400x300.jpg": "projects/out/img", "8-400x300.jpg": "projects/out/img", "9-400x300.jpg": "projects/out/img", "5-400x300.jpg": "projects/out/img", "7-400x300.jpg": "projects/out/img", "1-400x300.jpg": "projects/out/img", "2-400x300.jpg": "projects/out/img", "4-400x300.jpg": "projects/out/img"}, "_description": "A story about a boy in a imaginary world in which he communicates with his mother\n", "_body.textile": "h1. Out\n\nStories steered by problems must be clear in construction of the question at hand. This to make the used imagery and language recognizable to the user. This is to make it understandable to everyone whether it is the intention to reach the other side or to proceed to the center and that there are deliberately placed dead ends.\n\nEventually the user will know that the correct way to go is usually not the most logical way to go. This way of thinking play's an essential role in stories that deal with problems. More important then the problem itself is the way that the imagery of the problem is communicated to the user. A labyrinth gives an understandable image within this assignment.\n\nFor this project we were assinged to create a story based on 'Labyrinths'. We created a story about a boy in a imaginary world in which he communicates with his mother. His mother doesn't seem to hear him. We has to find it's way out by finding out what has happened to him. For this project we we're helped out by actors who did all the voice-overs.\n\nIt's hard to imagine how we made these animations with crappy computers back in the previous century. I remember that we rendered everything at 400x300 pixels because otherwise we would have never have finished the project. I kind of lost the video unfortunately.\n", "_status": "completed\n", "_title": "Out\n", "_date": "20000203\n"}, "eww": {"eww-inet.ssa": "projects/eww", "img": {"2.jpg": "projects/eww/img", "8.jpg": "projects/eww/img", "6.jpg": "projects/eww/img", "4.jpg": "projects/eww/img", "front01.jpg": "projects/eww/img", "7.jpg": "projects/eww/img", "5.jpg": "projects/eww/img", "3.jpg": "projects/eww/img", "1.jpg": "projects/eww/img", "9.jpg": "projects/eww/img"}, "_description": "A short film experiment by \"Koud uit de Koelkast Smeerbaar\"\n", "eww.video": "projects/eww", "_body.textile": "In this film you're following a character on his quest. Inside the tram he's telling about experiences he had in live. Through the telling of his experiences he explains what he's doing and why. A philosophical approach to our constant dualism; judgement about right and wrong and our fear towards not being in control.\n\nThe first production of \"Koud uit de Koelkast Smeerbaar\". An experiment to create a short film whithout knowing where it would end exactly. An 'open dramaturgic' approach to film making. After filming for 2 days we played with the material during the year and came up with this story. The story in spoken in Dutch. Thanks to Wreck@k-n-p.org the movie has optional \"English subtitles\":/projects/eww/eww-inet.ssa.\n", "_status": "completed\n", "_title": "De Emmer, het Water en de Waarheid\n", "_date": "20020303\n"}, "wecker": {"autoawesome.video": "projects/wecker", "img": {"01.png": {"web": "01_web.jpg", "thumb": "01_thumb.jpg", "md5": "todo"}, "00a.png": {"web": "00a_web.jpg", "thumb": "00a_thumb.jpg", "md5": "todo"}, "10.png": {"web": "10_web.jpg", "thumb": "10_thumb.jpg", "md5": "todo"}, "02.png": {"web": "02_web.jpg", "thumb": "02_thumb.jpg", "md5": "todo"}, "05.png": {"web": "05_web.jpg", "thumb": "05_thumb.jpg", "md5": "todo"}, "07.png": {"web": "07_web.jpg", "thumb": "07_thumb.jpg", "md5": "todo"}, "06.png": {"web": "06_web.jpg", "thumb": "06_thumb.jpg", "md5": "todo"}, "09.png": {"web": "09_web.jpg", "thumb": "09_thumb.jpg", "md5": "todo"}, "08.png": {"web": "08_web.jpg", "thumb": "08_thumb.jpg", "md5": "todo"}, "03.png": {"web": "03_web.jpg", "thumb": "03_thumb.jpg", "md5": "todo"}, "plaatje1.png": {"web": "plaatje1_web.jpg", "thumb": "plaatje1_thumb.jpg", "md5": "todo"}, "plaatje2.png": {"web": "plaatje2_web.jpg", "thumb": "plaatje2_thumb.jpg", "md5": "todo"}, "plaatje3.png": {"web": "plaatje3_web.jpg", "thumb": "plaatje3_thumb.jpg", "md5": "todo"}}, "_description": "Conserving an event's memories", "_body.textile": "h1. Oantinken\n\nOantinken is an interactive installation that conserves memories of an event in a huge wall of 'Weck' jars ('Kilner jars' in the UK and 'Mason jars' in the US). The project is a collaboration between \"Stichting z25.org\":http://www.z25.org and \"Tryater\":http://www.tryater.nl and based on Transmedial Storytelling concepts that were developed by my students of the \"Utrecht School for the Arts\":http://www.hku.nl.\n\n_Oantinken_ consists of two parts: a mobile app that asks you to capture certain moments by making a photo and a huge wall of jars where you can conserve these snapshots.\n\nThe images are amassed and placed in a new context. A kind of resonance is thus created, a recollection of what took place during the event. It will not be a balanced memory, but rather one manipulated by the collective.\n\nWill this manipulated image say anything about what really occurred? Is collective memory more reliable than the record in our own head?\n\nh3. Festivals and events\n\nThe installation was featured at various festivals and events.\n\n*Noorderzon Performing Arts Festival*\nFrom 15 to 25 August, Groningen\n\"More info\":http://www.noorderzon.nl/programma/programma-items/tryater-en-z25\n\n*Heimwee naar Hurdegaryp*\nFrom 21 January to 1 February, Leeuwarden\n\"More info\":http://www.tryater.nl/nederlands/voorstellingen/actueel/heimwee-nei-hurdegaryp/\n\nh3. Oantinken app for iOS and Android\n\n!{height:40px;}http://r.mzstatic.com/images/web/linkmaker/badge_appstore-lrg.gif!:https://itunes.apple.com/nl/app/oantinken/id649828938?mt=8&uo=4 !{height:40px;}https://developer.android.com/images/brand/en_generic_rgb_wo_45.png!:https://play.google.com/store/apps/details?id=org.z25.weckerapp\n\nh3. Oantinken Webfles (Webjar)\n\nA place where you can look back on the collected memories: \"www.oantinken.com\":http://www.oantinken.com\n\nh3. Credits\n\nBased on concepts by Niels Dielen, Jelle van Doorne, Anca Siegersma en Mick Vonk.\n\n* Rodrik Biersteker: Projectlead\n* Roderick Gadellaa: Software development\n* Ferdy Guliker: Support\n* Aldo Hoeben: Software development\n* Mark IJzerman: Sound design\n* Arnaud Loonstra: Software development\n* Machiel Veltkamp: Support\n* Wijnand Veneberg: Design\n", "_status": "completed", "_title": "Oantinken", "_date": "20130426\n"}, "broekpolder": {"img": {"01.jpg": "projects/broekpolder/img"}, "_description": "Online digital art project for the Broekpolder, a new housing development near Beverwijk and Heemskerk\n", "_body.textile": "h1. Broekpolder Orakel\n\nBroekpolder Orakel is an online digital art project for the Broekpolder, a new housing development near Beverwijk and Heemskerk. The artwork initiates a dialogue with the villagers of the Broekpolder. A dialogue at a different level of hope, expectations and dreams. The virtual world grows by interacting with its villagers and gives.\n\nIt is mostly referred to as a game but from my perspective it isn't. I haven't found a specific name for this genre but refer to it as an 'online multiuser experience'.\n\nThe project was available through the website \"http://www.kunstinbroekpolder.nl\":http://www.kunstinbroekpolder.nl until somewhere 2008.\n", "_status": "completed\n", "_title": "Broekpolder Orakel\n", "_date": "20060505\n"}, "schipahoy": {"img": {"airportpark_8.png": {"web": "airportpark_8_web.jpg", "thumb": "airportpark_8_thumb.jpg", "md5": "todo"}, "airportpark_9.png": {"web": "airportpark_9_web.jpg", "thumb": "airportpark_9_thumb.jpg", "md5": "todo"}, "airportpark_2.png": {"web": "airportpark_2_web.jpg", "thumb": "airportpark_2_thumb.jpg", "md5": "todo"}, "airportpark_3.png": {"web": "airportpark_3_web.jpg", "thumb": "airportpark_3_thumb.jpg", "md5": "todo"}, "airportpark_1.png": {"web": "airportpark_1_web.jpg", "thumb": "airportpark_1_thumb.jpg", "md5": "todo"}, "airportpark_6.png": {"web": "airportpark_6_web.jpg", "thumb": "airportpark_6_thumb.jpg", "md5": "todo"}, "airportpark_7.png": {"web": "airportpark_7_web.jpg", "thumb": "airportpark_7_thumb.jpg", "md5": "todo"}, "airportpark_4.png": {"web": "airportpark_4_web.jpg", "thumb": "airportpark_4_thumb.jpg", "md5": "todo"}, "airportpark_5.png": {"web": "airportpark_5_web.jpg", "thumb": "airportpark_5_thumb.jpg", "md5": "todo"}, "airportpark_10.png": {"web": "airportpark_10_web.jpg", "thumb": "airportpark_10_thumb.jpg", "md5": "todo"}}, "_description": "Airport Park is for everyone to enjoy. Once beyond passport control, passengers will now be able to spend their time in a surprising, park-themed area.\n", "_body.textile": "h1. Airport Park\n\nSchiphol has a new lounge where you find yourself in a park and meanwhile work a bit or enjoy the birds and (virtual)butterflies. \n\nFor this unique project I worked with \"z25\":http://z25.org, \"Talltree Labs\":http://talltreelabs.org and \"Geronimo\":http://bostochten.nl on two interactive installations and provided, in collaboration with Marcel Dolman, the sound scape for the park. Within this park at Schiphol Airport two locations are created with interactive projected butterflies that react to your presence. The butterflies are easy to scare away, but if you sit still and watch them quietly they will be curious to come close to you.\n\nThe installations incorporate some new mixed reality technologies and self control mechanisms which have been researched and developed by z25.\n\nPress  coverage:\n\n* \"NRC Next\":http://www.nrcnext.nl/columnisten/2011/05/11/airport-park-is-natuur-2-0-netjes-en-zonder-enge-beestjes/\n* \"Parool\":http://www.parool.nl/parool/nl/4/AMSTERDAM/article/detail/2421686/2011/05/10/Schiphol-opent-park.dhtml\n* \"RTV Noord Holland\":http://goo.gl/eIEC6\n* \"Telegraaf\":http://www.telegraaf.nl/reiskrant/9742405/__Prinses_Irene_opent_park_op_Schiphol__.html?sn=reiskrant\n* \"Architectenweb\":http://www.architectenweb.nl/aweb/redactie/redactie_detail.asp?iNID=26114\n", "_status": "completed", "_title": "Airport Park Schiphol", "_date": "20110517"}}, "about.textile": "\t<h1>About</h1>\n\n\t<p>Arnaud Loonstra is an artist, researcher and tinkerer curious about new possibilities with new technologies. He earned his Art & Technology bachelor at the Utrecht University for the Arts and his Master of Science degree at Leiden University. Dealing with software technology he is a strong advocate of the free and opensource software movement.</p>\n\n\t<p>He has leaded the <a href=\"http://www.z25.org\">z25.org Foundation</a> from 2007 to 2013 from which the <a href=\"http://maplab.nl\">MAPLAB</a> and <a href=\"http://www.setup.nl\">SETUP</a> organizations were brought to life.</p>\n\n\t<p>Arnaud is currently researching how to deal with modern software architecures for artistic purposes which he defines as &#8220;Code Orchestration&#8221;.</p>\n\n\t<p>In his free time he&#8217;s working on his Volkswagen T3 transporter unless there&#8217;s a breeze for windsurfing. </p>\n\n\t<p><a href=\"http://github.com/sphaero\">github</a> | <a href=\"arnaud@sphaero.org\">email</a> | <a href=\"xmpp://arnaud@z25.org\">xmpp</a> | <a href=\"http://twitter.com/sphaero\">twitter</a> | <a href=\"sphaero_knp\">irc</a> | <a href=\"https://www.linkedin.com/in/sphaero\">linkedin</a></p>"}